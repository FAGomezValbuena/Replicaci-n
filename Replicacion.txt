


MongoDB shell version v4.2.25
connecting to: mongodb://127.0.0.1:27017/?compressors=disabled&gssapiServiceName=mongodb
Implicit session: session { "id" : UUID("9e722c62-64d2-4749-8680-490e5e5d1bc0") }
MongoDB server version: 4.2.25
Server has startup warnings:
2024-05-24T13:02:43.544-0500 I  CONTROL  [initandlisten]
2024-05-24T13:02:43.545-0500 I  CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2024-05-24T13:02:43.545-0500 I  CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2024-05-24T13:02:43.556-0500 I  CONTROL  [initandlisten]
---
Enable MongoDB's free cloud-based monitoring service, which will then receive and display
metrics about your deployment (disk utilization, CPU, operation statistics, etc).

The monitoring data will be available on a MongoDB website with a unique URL accessible to you
and anyone you share the URL with. MongoDB may use this information to make product
improvements and to suggest MongoDB products and deployment options to you.

To enable free monitoring, run the following command: db.enableFreeMonitoring()
To permanently disable this reminder, run the following command: db.disableFreeMonitoring()
---

> show dbs
admin                   0.000GB
config                  0.000GB
local                   0.000GB
torneo_futbol_de_salon  0.000GB
> use torneo_futbol_de_salon
switched to db torneo_futbol_de_salon
> ejemploReplicaSet = new ReplSetTest({name:"MiReplicaSet", nodes : 3})
Starting new replica set MiReplicaSet
{
        "kDefaultTimeoutMS" : 600000,
        "getReadConcernMajorityOpTimeOrThrow" : function(conn) {
        const majorityOpTime = _getReadConcernMajorityOpTime(conn);
        if (friendlyEqual(majorityOpTime, {ts: Timestamp(0, 0), t: NumberLong(0)})) {
            throw new Error("readConcern majority optime not available");
        }
        return majorityOpTime;
    },
        "nodeList" : function() {
        var list = [];
        for (var i = 0; i < this.ports.length; i++) {
            list.push(this.host + ":" + this.ports[i]);
        }

        return list;
    },
        "getNodeId" : function(node) {
        if (node.toFixed) {
            return parseInt(node);
        }

        for (var i = 0; i < this.nodes.length; i++) {
            if (this.nodes[i] == node) {
                return i;
            }
        }

        if (node instanceof ObjectId) {
            for (i = 0; i < this.nodes.length; i++) {
                if (this.nodes[i].runId == node) {
                    return i;
                }
            }
        }

        if (node.nodeId != null) {
            return parseInt(node.nodeId);
        }

        return undefined;
    },
        "getPort" : function(n) {
        var n = this.getNodeId(n);
        return this.ports[n];
    },
        "getDbPath" : function(node) {
        // Get a replica set node (check for use of bridge).
        const n = this.getNodeId(node);
        const replNode = _useBridge ? _unbridgedNodes[n] : this.nodes[n];
        return replNode.dbpath;
    },
        "_addPath" : function(p) {
        if (!_alldbpaths)
            _alldbpaths = [p];
        else
            _alldbpaths.push(p);

        return p;
    },
        "getReplSetConfig" : function() {
        var cfg = {};
        cfg._id = this.name;
        cfg.protocolVersion = 1;

        cfg.members = [];

        for (var i = 0; i < this.ports.length; i++) {
            var member = {};
            member._id = i;

            member.host = this.host;
            if (!member.host.includes('/')) {
                member.host += ":" + this.ports[i];
            }

            var nodeOpts = this.nodeOptions["n" + i];
            if (nodeOpts) {
                if (nodeOpts.arbiter) {
                    member.arbiterOnly = true;
                }

                if (nodeOpts.rsConfig) {
                    Object.extend(member, nodeOpts.rsConfig);
                }
            }

            cfg.members.push(member);
        }

        if (_configSettings) {
            cfg.settings = _configSettings;
        }

        return cfg;
    },
        "getURL" : function() {
        var hosts = [];

        for (var i = 0; i < this.ports.length; i++) {
            hosts.push(this.host + ":" + this.ports[i]);
        }

        return this.name + "/" + hosts.join(",");
    },
        "startSet" : function(options, restart) {
        print("ReplSetTest starting set");

        if (options && options.keyFile) {
            self.keyFile = options.keyFile;
        }

        if (options) {
            self.startOptions = options;
        }

        var nodes = [];
        for (var n = 0; n < this.ports.length; n++) {
            nodes.push(this.start(n, options, restart));
        }

        this.nodes = nodes;
        return this.nodes;
    },
        "awaitSecondaryNodes" : function(timeout, slaves) {
        timeout = timeout || self.kDefaultTimeoutMS;

        assert.soonNoExcept(function() {
            // Reload who the current slaves are
            self.getPrimary(timeout);

            var slavesToCheck = slaves || self._slaves;
            var len = slavesToCheck.length;
            var ready = true;

            for (var i = 0; i < len; i++) {
                var isMaster = slavesToCheck[i].adminCommand({ismaster: 1});
                var arbiter = (isMaster.arbiterOnly === undefined ? false : isMaster.arbiterOnly);
                ready = ready && (isMaster.secondary || arbiter);
            }

            return ready;
        }, "Awaiting secondaries", timeout);
    },
        "awaitSecondaryNodesForRollbackTest" : function(
        timeout, slaves, connToCheckForUnrecoverableRollback) {
        try {
            this.awaitSecondaryNodes(timeout, slaves);
        } catch (originalEx) {
            // There is a special case where we expect the (rare) possibility of unrecoverable
            // rollbacks with EMRC:false in rollback suites with unclean shutdowns.
            jsTestLog("Exception in 'awaitSecondaryNodes', checking for unrecoverable rollback");
            if (connToCheckForUnrecoverableRollback) {
                const conn = connToCheckForUnrecoverableRollback;

                const statusRes = assert.commandWorked(conn.adminCommand({replSetGetStatus: 1}));
                const isRecovering = (statusRes.myState === ReplSetTest.State.RECOVERING);
                const hasNoSyncSource = (statusRes.syncSourceId === -1);

                const cmdLineOptsRes = assert.commandWorked(conn.adminCommand("getCmdLineOpts"));
                const hasEMRCFalse =
                    (cmdLineOptsRes.parsed.replication.enableMajorityReadConcern === false);

                if (isRecovering && hasNoSyncSource && hasEMRCFalse) {
                    try {
                        const n = this.getNodeId(conn);
                        const connToCheck = _useBridge ? _unbridgedNodes[n] : this.nodes[n];
                        // Confirm that the node is unable to recover after rolling back.
                        checkLog.contains(
                            connToCheck,
                            "remote oplog does not contain entry with optime matching our required optime ",
                            60 * 1000);
                    } catch (checkLogEx) {
                        throw originalEx;
                    }
                    // Add this info to the original exception.
                    originalEx.unrecoverableRollbackDetected = true;
                }
            }
            // Re-throw the original exception in all cases.
            throw originalEx;
        }
    },
        "awaitSyncSource" : function(node, upstreamNode, timeout) {
        print("Waiting for node " + node.name + " to start syncing from " + upstreamNode.name);
        var status = null;
        assert.soonNoExcept(
            function() {
                status = node.getDB("admin").runCommand({replSetGetStatus: 1});
                for (var j = 0; j < status.members.length; j++) {
                    if (status.members[j].self) {
                        return status.members[j].syncingTo === upstreamNode.host;
                    }
                }
                return false;
            },
            "Awaiting node " + node + " syncing from " + upstreamNode + ": " + tojson(status),
            timeout);
    },
        "awaitNodesAgreeOnAppliedOpTime" : function(timeout, nodes) {
        timeout = timeout || self.kDefaultTimeoutMS;
        nodes = nodes || self.nodes;

        assert.soon(function() {
            let appliedOpTimeConsensus = undefined;
            for (let i = 0; i < nodes.length; i++) {
                let replSetGetStatus;
                try {
                    replSetGetStatus = nodes[i].adminCommand({replSetGetStatus: 1});
                } catch (e) {
                    print("AwaitNodesAgreeOnAppliedOpTime: Retrying because node " + nodes[i].name +
                          " failed to execute replSetGetStatus: " + tojson(e));
                    return false;
                }
                assert.commandWorked(replSetGetStatus);

                if (appliedOpTimeConsensus === undefined) {
                    if (replSetGetStatus.optimes) {
                        appliedOpTimeConsensus = replSetGetStatus.optimes.appliedOpTime;
                    } else {
                        // Older versions of mongod do not include an 'optimes' field in the
                        // replSetGetStatus response. We instead pull an optime from the first
                        // replica set member that includes one in its status. All we need here is
                        // any initial value that we can compare to all the other optimes.
                        let optimeMembers = replSetGetStatus.members.filter(m => m.optime);
                        assert(optimeMembers.length > 0,
                               "AwaitNodesAgreeOnAppliedOpTime: replSetGetStatus did not " +
                                   "include optimes for any members: " + tojson(replSetGetStatus));
                        appliedOpTimeConsensus = optimeMembers[0].optime;
                    }

                    assert(appliedOpTimeConsensus,
                           "AwaitNodesAgreeOnAppliedOpTime: missing appliedOpTime in " +
                               "replSetGetStatus: " + tojson(replSetGetStatus));
                }

                if (replSetGetStatus.optimes &&
                    !friendlyEqual(replSetGetStatus.optimes.appliedOpTime,
                                   appliedOpTimeConsensus)) {
                    print("AwaitNodesAgreeOnAppliedOpTime: Retrying because node " + nodes[i].name +
                          " has appliedOpTime " + tojson(replSetGetStatus.optimes.appliedOpTime) +
                          " that does not match the previously observed appliedOpTime " +
                          tojson(appliedOpTimeConsensus));
                    return false;
                }

                for (let j = 0; j < replSetGetStatus.members.length; j++) {
                    if (replSetGetStatus.members[j].state == ReplSetTest.State.ARBITER) {
                        // ARBITER nodes do not apply oplog entries and do not have an 'optime'
                        // field.
                        continue;
                    }

                    if (!friendlyEqual(replSetGetStatus.members[j].optime,
                                       appliedOpTimeConsensus)) {
                        print("AwaitNodesAgreeOnAppliedOpTime: Retrying because node " +
                              nodes[i].name + " sees optime " +
                              tojson(replSetGetStatus.members[j].optime) + " on node " +
                              replSetGetStatus.members[j].name + " but expects to see optime " +
                              tojson(appliedOpTimeConsensus));
                        return false;
                    }
                }
            }

            print(
                "AwaitNodesAgreeOnAppliedOpTime: All nodes agree that all ops are applied up to " +
                tojson(appliedOpTimeConsensus));
            return true;
        }, "Awaiting nodes to agree that all ops are applied across replica set", timeout);
    },
        "_findHighestPriorityNodes" : function(config) {
        let highestPriority = 0;
        let highPriorityNodes = [];
        for (let i = 0; i < config.members.length; i++) {
            const member = config.members[i];
            if (member.priority > highestPriority) {
                highestPriority = member.priority;
                highPriorityNodes = [this.nodes[i]];
            } else if (member.priority === highestPriority) {
                highPriorityNodes.push(this.nodes[i]);
            }
        }
        return highPriorityNodes;
    },
        "awaitHighestPriorityNodeIsPrimary" : function(timeout) {
        timeout = timeout || self.kDefaultTimeoutMS;

        // First figure out the set of highest priority nodes.
        const config = asCluster(this.nodes, () => self.getReplSetConfigFromNode());
        const highPriorityNodes = this._findHighestPriorityNodes(config);

        // Now wait for the primary to be one of the highest priority nodes.
        assert.soon(
            function() {
                return highPriorityNodes.includes(self.getPrimary());
            },
            function() {
                return "Expected primary to be one of: " + tojson(highPriorityNodes) +
                    ", but found primary to be: " + tojson(self.getPrimary());
            },
            timeout);

        // Finally wait for all nodes to agree on the primary.
        this.awaitNodesAgreeOnPrimary(timeout);
        const primary = this.getPrimary();
        assert(highPriorityNodes.includes(primary),
               "Primary switched away from highest priority node.  Found primary: " +
                   tojson(primary) + ", but expected one of: " + tojson(highPriorityNodes));
    },
        "awaitNodesAgreeOnPrimary" : function(timeout, nodes, expectedPrimaryNodeId) {
        timeout = timeout || self.kDefaultTimeoutMS;
        nodes = nodes || self.nodes;
        expectedPrimaryNodeId = expectedPrimaryNodeId || -1;
        if (expectedPrimaryNodeId === -1) {
            print("AwaitNodesAgreeOnPrimary: Waiting for nodes to agree on any primary.");
        } else {
            print("AwaitNodesAgreeOnPrimary: Waiting for nodes to agree on " +
                  nodes[expectedPrimaryNodeId].name + " as primary.");
        }

        assert.soonNoExcept(function() {
            var primary = expectedPrimaryNodeId;

            for (var i = 0; i < nodes.length; i++) {
                var replSetGetStatus =
                    assert.commandWorked(nodes[i].getDB("admin").runCommand({replSetGetStatus: 1}));
                var nodesPrimary = -1;
                for (var j = 0; j < replSetGetStatus.members.length; j++) {
                    if (replSetGetStatus.members[j].state === ReplSetTest.State.PRIMARY) {
                        // Node sees two primaries.
                        if (nodesPrimary !== -1) {
                            print("AwaitNodesAgreeOnPrimary: Retrying because " + nodes[i].name +
                                  " thinks both " + self.nodes[nodesPrimary].name + " and " +
                                  self.nodes[j].name + " are primary.");

                            return false;
                        }
                        nodesPrimary = j;
                    }
                }
                // Node doesn't see a primary.
                if (nodesPrimary < 0) {
                    print("AwaitNodesAgreeOnPrimary: Retrying because " + nodes[i].name +
                          " does not see a primary.");
                    return false;
                }

                if (primary < 0) {
                    // If we haven't seen a primary yet, set it to this.
                    primary = nodesPrimary;
                } else if (primary !== nodesPrimary) {
                    print("AwaitNodesAgreeOnPrimary: Retrying because " + nodes[i].name +
                          " thinks the primary is " + self.nodes[nodesPrimary].name +
                          " instead of " + self.nodes[primary].name);
                    return false;
                }
            }

            print("AwaitNodesAgreeOnPrimary: Nodes agreed on primary " + self.nodes[primary].name);
            return true;
        }, "Awaiting nodes to agree on primary timed out", timeout);
    },
        "getPrimary" : function(timeout) {
        timeout = timeout || self.kDefaultTimeoutMS;
        var primary = null;

        assert.soonNoExcept(function() {
            primary = _callIsMaster();
            return primary;
        }, "Finding primary", timeout);

        return primary;
    },
        "awaitNoPrimary" : function(msg, timeout) {
        msg = msg || "Timed out waiting for there to be no primary in replset: " + this.name;
        timeout = timeout || self.kDefaultTimeoutMS;

        assert.soonNoExcept(function() {
            return _callIsMaster() == false;
        }, msg, timeout);
    },
        "getSecondaries" : function(timeout) {
        var master = this.getPrimary(timeout);
        var secs = [];
        for (var i = 0; i < this.nodes.length; i++) {
            if (this.nodes[i] != master) {
                secs.push(this.nodes[i]);
            }
        }

        return secs;
    },
        "getSecondary" : function(timeout) {
        return this.getSecondaries(timeout)[0];
    },
        "getArbiters" : function() {
        let arbiters = [];
        for (let i = 0; i < this.nodes.length; i++) {
            const node = this.nodes[i];

            let isArbiter = false;

            assert.retryNoExcept(() => {
                isArbiter = isNodeArbiter(node);
                return true;
            }, `Could not call 'isMaster' on ${node}.`, 3, 1000);

            if (isArbiter) {
                arbiters.push(node);
            }
        }
        return arbiters;
    },
        "getArbiter" : function() {
        return this.getArbiters()[0];
    },
        "status" : function(timeout) {
        var master = _callIsMaster();
        if (!master) {
            master = this._liveNodes[0];
        }

        return master.getDB("admin").runCommand({replSetGetStatus: 1});
    },
        "add" : function(config) {
        var nextPort = _allocatePortForNode();
        print("ReplSetTest Next port: " + nextPort);

        this.ports.push(nextPort);
        printjson(this.ports);

        if (_useBridge) {
            _unbridgedPorts.push(_allocatePortForBridge());
        }

        var nextId = this.nodes.length;
        printjson(this.nodes);

        print("ReplSetTest nextId: " + nextId);
        return this.start(nextId, config);
    },
        "remove" : function(nodeId) {
        nodeId = this.getNodeId(nodeId);
        this.nodes.splice(nodeId, 1);
        this.ports.splice(nodeId, 1);

        if (_useBridge) {
            _unbridgedPorts.splice(nodeId, 1);
            _unbridgedNodes.splice(nodeId, 1);
        }
    },
        "_updateConfigIfNotDurable" : function(config) {
        // Get a replica set node (check for use of bridge).
        var replNode = _useBridge ? _unbridgedNodes[0] : this.nodes[0];

        // Don't update replset config for sharding config servers since config servers always
        // require durable storage.
        if (replNode.hasOwnProperty("fullOptions") &&
            replNode.fullOptions.hasOwnProperty("configsvr")) {
            return config;
        }

        // Don't override existing value.
        var wcMajorityJournalField = "writeConcernMajorityJournalDefault";
        if (config.hasOwnProperty(wcMajorityJournalField)) {
            return config;
        }

        // Check journaling by sending commands through the bridge if it's used.
        if (_isRunningWithoutJournaling(this.nodes[0])) {
            config[wcMajorityJournalField] = false;
        }

        return config;
    },
        "_setDefaultConfigOptions" : function(config) {
        // Update config for non journaling test variants
        this._updateConfigIfNotDurable(config);
        // Add protocolVersion if missing
        if (!config.hasOwnProperty('protocolVersion')) {
            config['protocolVersion'] = 1;
        }
    },
        "initiateWithAnyNodeAsPrimary" : function(cfg, initCmd, {
        doNotWaitForStableRecoveryTimestamp: doNotWaitForStableRecoveryTimestamp = false
    } = {}) {
        var master = this.nodes[0].getDB("admin");
        var config = cfg || this.getReplSetConfig();
        var cmd = {};
        var cmdKey = initCmd || 'replSetInitiate';

        // Throw an exception if nodes[0] is unelectable in the given config.
        if (!_isElectable(config.members[0])) {
            throw Error("The node at index 0 must be electable");
        }

        // Start up a single node replica set then reconfigure to the correct size (if the config
        // contains more than 1 node), so the primary is elected more quickly.
        var originalMembers, originalSettings;
        if (config.members && config.members.length > 1) {
            originalMembers = config.members.slice();
            config.members = config.members.slice(0, 1);
            originalSettings = config.settings;
            delete config.settings;  // Clear settings to avoid tags referencing sliced nodes.
        }
        this._setDefaultConfigOptions(config);

        cmd[cmdKey] = config;

        // replSetInitiate and replSetReconfig commands can fail with a NodeNotFound error if a
        // heartbeat times out during the quorum check. They may also fail with
        // NewReplicaSetConfigurationIncompatible on similar timeout during the config validation
        // stage while deducing isSelf(). This can fail with an InterruptedDueToReplStateChange
        // error when interrupted. We try several times, to reduce the chance of failing this way.
        replSetCommandWithRetry(master, cmd);
        this.getPrimary();  // Blocks until there is a primary.

        // Reconfigure the set to contain the correct number of nodes (if necessary).
        if (originalMembers) {
            config.members = originalMembers;
            if (originalSettings) {
                config.settings = originalSettings;
            }
            config.version = 2;

            // Nodes started with the --configsvr flag must have configsvr = true in their config.
            if (this.nodes[0].hasOwnProperty("fullOptions") &&
                this.nodes[0].fullOptions.hasOwnProperty("configsvr")) {
                config.configsvr = true;
            }

            cmd = {replSetReconfig: config};
            print("Reconfiguring replica set to add in other nodes");
            replSetCommandWithRetry(master, cmd);
        }

        // Setup authentication if running test with authentication
        if ((jsTestOptions().keyFile) && cmdKey == 'replSetInitiate') {
            master = this.getPrimary();
            jsTest.authenticateNodes(this.nodes);
        }
        this.awaitSecondaryNodes();
        try {
            this.awaitHighestPriorityNodeIsPrimary();
        } catch (e) {
            // Due to SERVER-14017, the call to awaitHighestPriorityNodeIsPrimary() may fail
            // in certain configurations due to being unauthorized.  In that case we proceed
            // even though we aren't guaranteed that the highest priority node is the one that
            // became primary.
            // TODO(SERVER-14017): Unconditionally expect awaitHighestPriorityNodeIsPrimary to pass.
            assert.eq(ErrorCodes.Unauthorized, e.code, tojson(e));
            print("Running awaitHighestPriorityNodeIsPrimary() during ReplSetTest initialization " +
                  "failed with Unauthorized error, proceeding even though we aren't guaranteed " +
                  "that the highest priority node is primary");
        }

        let shouldWaitForKeys = true;
        if (self.waitForKeys != undefined) {
            shouldWaitForKeys = self.waitForKeys;
            print("Set shouldWaitForKeys from RS options: " + shouldWaitForKeys);
        } else {
            Object.keys(self.nodeOptions).forEach(function(key, index) {
                let val = self.nodeOptions[key];
                if (typeof (val) === "object" &&
                    (val.hasOwnProperty("shardsvr") ||
                     val.hasOwnProperty("binVersion") &&
                         // Should not wait for keys if version is less than 3.6
                         MongoRunner.compareBinVersions(val.binVersion, "3.6") == -1)) {
                    shouldWaitForKeys = false;
                    print("Set shouldWaitForKeys from node options: " + shouldWaitForKeys);
                }
            });
            if (self.startOptions != undefined) {
                let val = self.startOptions;
                if (typeof (val) === "object" &&
                    (val.hasOwnProperty("shardsvr") ||
                     val.hasOwnProperty("binVersion") &&
                         // Should not wait for keys if version is less than 3.6
                         MongoRunner.compareBinVersions(val.binVersion, "3.6") == -1)) {
                    shouldWaitForKeys = false;
                    print("Set shouldWaitForKeys from start options: " + shouldWaitForKeys);
                }
            }
        }
        /**
         * Blocks until the primary node generates cluster time sign keys.
         */
        if (shouldWaitForKeys) {
            var timeout = self.kDefaultTimeoutMS;
            asCluster(this.nodes, function(timeout) {
                print("Waiting for keys to sign $clusterTime to be generated");
                assert.soonNoExcept(function(timeout) {
                    var keyCnt = self.getPrimary(timeout)
                                     .getCollection('admin.system.keys')
                                     .find({purpose: 'HMAC'})
                                     .itcount();
                    return keyCnt >= 2;
                }, "Awaiting keys", timeout);
            });
        }

        // Set 'featureCompatibilityVersion' for the entire replica set, if specified.
        if (jsTest.options().replSetFeatureCompatibilityVersion) {
            // Authenticate before running the command.
            asCluster(self.nodes, function setFCV() {
                let fcv = jsTest.options().replSetFeatureCompatibilityVersion;
                print("Setting feature compatibility version for replica set to '" + fcv + "'");
                assert.commandWorked(
                    self.getPrimary().adminCommand({setFeatureCompatibilityVersion: fcv}));

                // Wait for the new 'featureCompatibilityVersion' to propagate to all nodes in the
                // replica set. The 'setFeatureCompatibilityVersion' command only waits for
                // replication to a majority of nodes by default.
                self.awaitReplication();
            });
        }

        if (!doNotWaitForStableRecoveryTimestamp) {
            self.awaitLastStableRecoveryTimestamp();
        }
    },
        "initiateWithNodeZeroAsPrimary" : function(cfg, initCmd) {
        this.initiateWithAnyNodeAsPrimary(cfg, initCmd);

        // stepUp() calls awaitReplication() which requires all nodes to be authorized to run
        // replSetGetStatus.
        asCluster(this.nodes, function() {
            self.stepUp(self.nodes[0]);
        });
    },
        "initiate" : function(cfg, initCmd) {
        this.initiateWithNodeZeroAsPrimary(cfg, initCmd);
    },
        "initiateWithHighElectionTimeout" : function(config) {
        config = config || this.getReplSetConfig();
        config.settings = config.settings || {};
        config.settings["electionTimeoutMillis"] = ReplSetTest.kForeverMillis;
        this.initiate(config);
    },
        "stepUp" : function(node, {
        awaitReplicationBeforeStepUp: awaitReplicationBeforeStepUp = true,
        awaitWritablePrimary: awaitWritablePrimary = true
    } = {}) {
        jsTest.log("ReplSetTest stepUp: Stepping up " + node.host);

        if (awaitReplicationBeforeStepUp) {
            this.awaitReplication();
        }

        assert.soonNoExcept(() => {
            const res = node.adminCommand({replSetStepUp: 1});
            // This error is possible if we are running mongoDB binary < 3.4 as
            // part of multi-version upgrade test. So, for those older branches,
            // simply wait for the requested node to get elected as primary due
            // to election timeout.
            if (!res.ok && res.code === ErrorCodes.CommandNotFound) {
                jsTest.log(
                    'replSetStepUp command not supported on node ' + node.host +
                    " ; so wait for the requested node to get elected due to election timeout.");
                if (this.getPrimary() === node) {
                    return true;
                }
            }
            assert.commandWorked(res);

            // Since assert.soon() timeout is 10 minutes (default), setting
            // awaitNodesAgreeOnPrimary() timeout as 1 minute to allow retry of replSetStepUp
            // command on failure of the replica set to agree on the primary.
            const timeout = 60 * 1000;
            this.awaitNodesAgreeOnPrimary(timeout, this.nodes, this.getNodeId(node));

            // getPrimary() guarantees that there will be only one writable primary for a replica
            // set.
            if (!awaitWritablePrimary || this.getPrimary() === node) {
                return true;
            }

            jsTest.log(node.host + ' is not primary after stepUp command');
            return false;
        }, "Timed out while waiting for stepUp to succeed on node in port: " + node.port);

        jsTest.log("ReplSetTest stepUp: Finished stepping up " + node.host);
        return node;
    },
        "getReplSetConfigFromNode" : function(nodeId) {
        if (nodeId == undefined) {
            // Use 90 seconds timeout for finding a primary
            return _replSetGetConfig(self.getPrimary(90 * 1000));
        }

        if (!isNumber(nodeId)) {
            throw Error(nodeId + ' is not a number');
        }

        return _replSetGetConfig(self.nodes[nodeId]);
    },
        "reInitiate" : function() {
        var config = this.getReplSetConfigFromNode();
        var newConfig = this.getReplSetConfig();
        // Only reset members.
        config.members = newConfig.members;
        config.version += 1;

        this._setDefaultConfigOptions(config);

        assert.adminCommandWorkedAllowingNetworkError(this.getPrimary(), {replSetReconfig: config});
    },
        "awaitNodesAgreeOnConfigVersion" : function(timeout) {
        timeout = timeout || this.kDefaultTimeoutMS;

        assert.soonNoExcept(function() {
            var primaryVersion = self.getPrimary().adminCommand({ismaster: 1}).setVersion;

            for (var i = 0; i < self.nodes.length; i++) {
                var version = self.nodes[i].adminCommand({ismaster: 1}).setVersion;
                assert.eq(version,
                          primaryVersion,
                          "waiting for secondary node " + self.nodes[i].host +
                              " with config version of " + version +
                              " to match the version of the primary " + primaryVersion);
            }

            return true;
        }, "Awaiting nodes to agree on config version", timeout);
    },
        "awaitLastOpCommitted" : function(timeout, members) {
        var rst = this;
        var master = rst.getPrimary();
        var masterOpTime = _getLastOpTime(master);

        let membersToCheck;
        if (members !== undefined) {
            print("Waiting for op with OpTime " + tojson(masterOpTime) + " to be committed on " +
                  members.map(s => s.host));

            membersToCheck = members;
        } else {
            print("Waiting for op with OpTime " + tojson(masterOpTime) +
                  " to be committed on all secondaries");

            membersToCheck = rst.nodes;
        }

        assert.soonNoExcept(
            function() {
                for (var i = 0; i < membersToCheck.length; i++) {
                    var node = membersToCheck[i];

                    // Continue if we're connected to an arbiter
                    var res = assert.commandWorked(node.adminCommand({replSetGetStatus: 1}));
                    if (res.myState == ReplSetTest.State.ARBITER) {
                        continue;
                    }
                    var rcmOpTime = _getReadConcernMajorityOpTime(node);
                    if (friendlyEqual(rcmOpTime, {ts: Timestamp(0, 0), t: NumberLong(0)})) {
                        return false;
                    }
                    if (rs.compareOpTimes(rcmOpTime, masterOpTime) < 0) {
                        return false;
                    }
                }

                return true;
            },
            "Op with OpTime " + tojson(masterOpTime) + " failed to be committed on all secondaries",
            timeout);

        return masterOpTime;
    },
        "awaitLastStableRecoveryTimestamp" : function() {
        let rst = this;
        let master = rst.getPrimary();
        let id = tojson(rst.nodeList());

        // All nodes must be in primary/secondary state prior to this point. Perform a majority
        // write to ensure there is a committed operation on the set. The commit point will
        // propagate to all members and trigger a stable checkpoint on all persisted storage engines
        // nodes.
        function advanceCommitPoint(master) {
            // Shadow 'db' so that we can call 'advanceCommitPoint' directly on the primary node.
            let db = master.getDB('admin');
            const appendOplogNoteFn = function() {
                assert.commandWorked(db.adminCommand({
                    "appendOplogNote": 1,
                    "data": {"awaitLastStableRecoveryTimestamp": 1},
                    "writeConcern": {"w": "majority", "wtimeout": ReplSetTest.kDefaultTimeoutMS}
                }));

                // Perform a second write, in case some node had lost its sync source.
                // TODO SERVER-40211: Remove the second write.
                assert.commandWorked(db.adminCommand(
                    {"appendOplogNote": 1, "data": {"awaitLastStableRecoveryTimestamp": 2}}));
            };

            // TODO(SERVER-14017): Remove this extra sub-shell in favor of a cleaner authentication
            // solution.
            const masterId = "n" + rst.getNodeId(master);
            const masterOptions = rst.nodeOptions[masterId] || {};
            if (masterOptions.clusterAuthMode === "x509") {
                print("AwaitLastStableRecoveryTimestamp: authenticating on separate shell " +
                      "with x509 for " + id);
                const subShellArgs = [
                    'mongo',
                    '--ssl',
                    '--sslCAFile=' + masterOptions.sslCAFile,
                    '--sslPEMKeyFile=' + masterOptions.sslPEMKeyFile,
                    '--sslAllowInvalidHostnames',
                    '--authenticationDatabase=$external',
                    '--authenticationMechanism=MONGODB-X509',
                    master.host,
                    '--eval',
                    `(${appendOplogNoteFn.toString()})();`
                ];

                const retVal = _runMongoProgram(...subShellArgs);
                assert.eq(retVal, 0, 'mongo shell did not succeed with exit code 0');
            } else {
                if (masterOptions.clusterAuthMode) {
                    print("AwaitLastStableRecoveryTimestamp: authenticating with " +
                          masterOptions.clusterAuthMode + " for " + id);
                }
                asCluster(master, appendOplogNoteFn, masterOptions.keyFile);
            }
        }

        print("AwaitLastStableRecoveryTimestamp: Beginning for " + id);

        let replSetStatus = assert.commandWorked(master.adminCommand("replSetGetStatus"));
        if (replSetStatus["configsvr"]) {
            // Performing dummy replicated writes against a configsvr is hard, especially if auth
            // is also enabled.
            return;
        }

        rst.awaitNodesAgreeOnPrimary();
        master = rst.getPrimary();

        print("AwaitLastStableRecoveryTimestamp: ensuring the commit point advances for " + id);
        advanceCommitPoint(master);

        print("AwaitLastStableRecoveryTimestamp: Waiting for stable recovery timestamps for " + id);

        assert.soonNoExcept(function() {
            for (let node of rst.nodes) {
                // The `lastStableRecoveryTimestamp` field contains a stable timestamp guaranteed to
                // exist on storage engine recovery to stable timestamp.
                let res = assert.commandWorked(node.adminCommand({replSetGetStatus: 1}));

                // Continue if we're connected to an arbiter.
                if (res.myState === ReplSetTest.State.ARBITER) {
                    continue;
                }

                // A missing `lastStableRecoveryTimestamp` field indicates that the storage
                // engine does not support `recover to a stable timestamp`.
                //
                // A null `lastStableRecoveryTimestamp` indicates that the storage engine supports
                // "recover to a stable timestamp", but does not have a stable recovery timestamp
                // yet.
                if (res.hasOwnProperty("lastStableRecoveryTimestamp") &&
                    res.lastStableRecoveryTimestamp.getTime() === 0) {
                    print("AwaitLastStableRecoveryTimestamp: " + node.host +
                          " does not have a stable recovery timestamp yet.");
                    return false;
                }

                // The `lastStableCheckpointTimestamp` field was added in v4.0, then deprecated and
                // replaced in v4.2 with `lastStableRecoveryTimestamp`. So we check it, too, for
                // backwards compatibility with v4.0 mongods.
                if (res.hasOwnProperty("lastStableCheckpointTimestamp") &&
                    res.lastStableCheckpointTimestamp.getTime() === 0) {
                    print("AwaitLastStableRecoveryTimestamp: " + node.host +
                          " does not have a stable recovery (checkpoint) timestamp yet.");
                    return false;
                }
            }

            return true;
        }, "Not all members have a stable recovery timestamp");

        print("AwaitLastStableRecoveryTimestamp: A stable recovery timestamp has successfully " +
              "established on " + id);
    },
        "awaitReplication" : function(timeout, secondaryOpTimeType, slaves) {
        if (slaves !== undefined && slaves !== self._slaves) {
            print("ReplSetTest awaitReplication: going to check only " + slaves.map(s => s.host));
        }

        timeout = timeout || self.kDefaultTimeoutMS;

        secondaryOpTimeType = secondaryOpTimeType || ReplSetTest.OpTimeType.LAST_APPLIED;

        var masterLatestOpTime;

        // Blocking call, which will wait for the last optime written on the master to be available
        var awaitLastOpTimeWrittenFn = function() {
            var master = self.getPrimary();
            assert.soonNoExcept(function() {
                try {
                    masterLatestOpTime = _getLastOpTime(master);
                } catch (e) {
                    print("ReplSetTest caught exception " + e);
                    return false;
                }

                return true;
            }, "awaiting oplog query", timeout);
        };

        awaitLastOpTimeWrittenFn();

        // get the latest config version from master (with a few retries in case of error)
        var masterConfigVersion;
        var masterName;
        var master;
        var num_attempts = 3;

        assert.retryNoExcept(() => {
            master = this.getPrimary();
            masterConfigVersion = this.getReplSetConfigFromNode().version;
            masterName = master.host;
            return true;
        }, "ReplSetTest awaitReplication: couldnt get repl set config.", num_attempts, 1000);

        print("ReplSetTest awaitReplication: starting: optime for primary, " + masterName +
              ", is " + tojson(masterLatestOpTime));

        let nodesCaughtUp = false;
        let slavesToCheck = slaves || self._slaves;
        let nodeProgress = Array(slavesToCheck.length);

        const Progress = Object.freeze({
            Skip: 'Skip',
            CaughtUp: 'CaughtUp',
            InProgress: 'InProgress',
            Stuck: 'Stuck',
            ConfigMismatch: 'ConfigMismatch'
        });

        function checkProgressSingleNode(index, secondaryCount) {
            var slave = slavesToCheck[index];
            var slaveName = slave.host;

            var slaveConfigVersion = slave.getDB("local")['system.replset'].findOne().version;

            if (masterConfigVersion != slaveConfigVersion) {
                print("ReplSetTest awaitReplication: secondary #" + secondaryCount + ", " +
                      slaveName + ", has config version #" + slaveConfigVersion +
                      ", but expected config version #" + masterConfigVersion);

                if (slaveConfigVersion > masterConfigVersion) {
                    master = self.getPrimary();
                    masterConfigVersion = master.getDB("local")['system.replset'].findOne().version;
                    masterName = master.host;

                    print("ReplSetTest awaitReplication: optime for primary, " + masterName +
                          ", is " + tojson(masterLatestOpTime));
                }

                return Progress.ConfigMismatch;
            }

            // Skip this node if we're connected to an arbiter
            var res = assert.commandWorked(slave.adminCommand({replSetGetStatus: 1}));
            if (res.myState == ReplSetTest.State.ARBITER) {
                return Progress.Skip;
            }

            print("ReplSetTest awaitReplication: checking secondary #" + secondaryCount + ": " +
                  slaveName);

            slave.getDB("admin").getMongo().setSecondaryOk();

            var slaveOpTime;
            if (secondaryOpTimeType == ReplSetTest.OpTimeType.LAST_DURABLE) {
                slaveOpTime = _getDurableOpTime(slave);
            } else {
                slaveOpTime = _getLastOpTime(slave);
            }

            // If the node doesn't have a valid opTime, it likely hasn't received any writes from
            // the primary yet.
            if (!rs.isValidOpTime(slaveOpTime)) {
                print("ReplSetTest awaitReplication: optime for secondary #" + secondaryCount +
                      ", " + slaveName + ", is " + tojson(slaveOpTime) + ", which is NOT valid.");
                return Progress.Stuck;
            }

            // See if the node made progress. We count it as progress even if the node's last optime
            // went backwards because that means the node is in rollback.
            let madeProgress =
                (nodeProgress[index] && (rs.compareOpTimes(nodeProgress[index], slaveOpTime) != 0));
            nodeProgress[index] = slaveOpTime;

            if (rs.compareOpTimes(masterLatestOpTime, slaveOpTime) < 0) {
                masterLatestOpTime = _getLastOpTime(master);
                print("ReplSetTest awaitReplication: optime for " + slaveName +
                      " is newer, resetting latest primary optime to " +
                      tojson(masterLatestOpTime) + ". Also resetting awaitReplication timeout");
                return Progress.InProgress;
            }

            if (!friendlyEqual(masterLatestOpTime, slaveOpTime)) {
                print("ReplSetTest awaitReplication: optime for secondary #" + secondaryCount +
                      ", " + slaveName + ", is " + tojson(slaveOpTime) + " but latest is " +
                      tojson(masterLatestOpTime));
                print("ReplSetTest awaitReplication: secondary #" + secondaryCount + ", " +
                      slaveName + ", is NOT synced");

                // Reset the timeout if a node makes progress, but isn't caught up yet.
                if (madeProgress) {
                    print("ReplSetTest awaitReplication: secondary #" + secondaryCount + ", " +
                          slaveName + ", has made progress. Resetting awaitReplication timeout");
                    return Progress.InProgress;
                }
                return Progress.Stuck;
            }

            print("ReplSetTest awaitReplication: secondary #" + secondaryCount + ", " + slaveName +
                  ", is synced");
            return Progress.CaughtUp;
        }

        // We will reset the timeout if a nodes makes progress, but still isn't caught up yet.
        while (!nodesCaughtUp) {
            assert.soonNoExcept(function() {
                try {
                    print("ReplSetTest awaitReplication: checking secondaries against latest " +
                          "primary optime " + tojson(masterLatestOpTime));
                    var secondaryCount = 0;

                    for (var i = 0; i < slavesToCheck.length; i++) {
                        const action = checkProgressSingleNode(i, secondaryCount);

                        switch (action) {
                            case Progress.CaughtUp:
                                // We only need to increment the secondaryCount if this node is
                                // caught up.
                                secondaryCount++;
                                continue;
                            case Progress.Skip:
                                // Don't increment secondaryCount because this node is an arbiter.
                                continue;
                            case Progress.InProgress:
                                return true;
                            case Progress.Stuck:
                            case Progress.ConfigMismatch:
                                return false;
                            default:
                                throw Error("invalid action: " + tojson(action));
                        }
                    }

                    print("ReplSetTest awaitReplication: finished: all " + secondaryCount +
                          " secondaries synced at optime " + tojson(masterLatestOpTime));
                    nodesCaughtUp = true;
                    return true;
                } catch (e) {
                    print("ReplSetTest awaitReplication: caught exception " + e);

                    // We might have a new master now
                    awaitLastOpTimeWrittenFn();

                    print("ReplSetTest awaitReplication: resetting: optime for primary " +
                          self._master + " is " + tojson(masterLatestOpTime));

                    return false;
                }
            }, "awaiting replication", timeout);
        }
    },
        "waitForAllIndexBuildsToFinish" : function(dbName, collName) {
        // Run a no-op command and wait for it to be applied on secondaries. Due to the asynchronous
        // completion nature of indexes on secondaries, we can guarantee an index build is complete
        // on all secondaries once all secondaries have applied this collMod command.
        assert.commandWorked(this.getPrimary().getDB(dbName).runCommand({collMod: collName}));
        this.awaitReplication();
    },
        "getHashesUsingSessions" : function(sessions, dbName, {
        filterCapped: filterCapped = true,
        filterMapReduce: filterMapReduce = true,
        readAtClusterTime,
    } = {}) {
        return sessions.map(session => {
            const commandObj = {dbHash: 1};
            if (readAtClusterTime !== undefined) {
                commandObj.$_internalReadAtClusterTime = readAtClusterTime;
            }

            const db = session.getDatabase(dbName);
            const res = assert.commandWorked(db.runCommand(commandObj));

            // The "capped" field in the dbHash command response is new as of MongoDB 4.0.
            const cappedCollections = new Set(filterCapped ? res.capped : []);

            for (let collName of Object.keys(res.collections)) {
                // Capped collections are not necessarily truncated at the same points across
                // replica set members and may therefore not have the same md5sum. We remove them
                // from the dbHash command response to avoid an already known case of a mismatch.
                // See SERVER-16049 for more details.
                //
                // If a map-reduce operation is interrupted by the server stepping down, then an
                // unreplicated "tmp.mr." collection may be left behind. We remove it from the
                // dbHash command response to avoid an already known case of a mismatch.
                // TODO SERVER-27147: Stop filtering out "tmp.mr." collections.
                if (cappedCollections.has(collName) ||
                    (filterMapReduce && collName.startsWith("tmp.mr."))) {
                    delete res.collections[collName];
                    // The "uuids" field in the dbHash command response is new as of MongoDB 4.0.
                    if (res.hasOwnProperty("uuids")) {
                        delete res.uuids[collName];
                    }
                }
            }

            return res;
        });
    },
        "getCollectionDiffUsingSessions" : function(
        primarySession, secondarySession, dbName, collNameOrUUID, readAtClusterTime) {
        function PeekableCursor(cursor) {
            let _stashedDoc;

            this.hasNext = function hasNext() {
                return cursor.hasNext();
            };

            this.peekNext = function peekNext() {
                if (_stashedDoc === undefined) {
                    _stashedDoc = cursor.next();
                }
                return _stashedDoc;
            };

            this.next = function next() {
                const result = (_stashedDoc === undefined) ? cursor.next() : _stashedDoc;
                _stashedDoc = undefined;
                return result;
            };
        }

        const docsWithDifferentContents = [];
        const docsMissingOnPrimary = [];
        const docsMissingOnSecondary = [];

        const primaryDB = primarySession.getDatabase(dbName);
        const secondaryDB = secondarySession.getDatabase(dbName);

        const commandObj = {find: collNameOrUUID, sort: {_id: 1}};
        if (readAtClusterTime !== undefined) {
            commandObj.$_internalReadAtClusterTime = readAtClusterTime;
        }

        const primaryCursor =
            new PeekableCursor(new DBCommandCursor(primaryDB, primaryDB.runCommand(commandObj)));

        const secondaryCursor = new PeekableCursor(
            new DBCommandCursor(secondaryDB, secondaryDB.runCommand(commandObj)));

        while (primaryCursor.hasNext() && secondaryCursor.hasNext()) {
            const primaryDoc = primaryCursor.peekNext();
            const secondaryDoc = secondaryCursor.peekNext();

            if (bsonBinaryEqual(primaryDoc, secondaryDoc)) {
                // The same document was found on the primary and secondary so we just move on to
                // the next document for both cursors.
                primaryCursor.next();
                secondaryCursor.next();
                continue;
            }

            const ordering = bsonWoCompare({_: primaryDoc._id}, {_: secondaryDoc._id});
            if (ordering === 0) {
                // The documents have the same _id but have different contents.
                docsWithDifferentContents.push({primary: primaryDoc, secondary: secondaryDoc});
                primaryCursor.next();
                secondaryCursor.next();
            } else if (ordering < 0) {
                // The primary's next document has a smaller _id than the secondary's next document.
                // Since we are iterating the documents in ascending order by their _id, we'll never
                // see a document with 'primaryDoc._id' on the secondary.
                docsMissingOnSecondary.push(primaryDoc);
                primaryCursor.next();
            } else if (ordering > 0) {
                // The primary's next document has a larger _id than the secondary's next document.
                // Since we are iterating the documents in ascending order by their _id, we'll never
                // see a document with 'secondaryDoc._id' on the primary.
                docsMissingOnPrimary.push(secondaryDoc);
                secondaryCursor.next();
            }
        }

        while (primaryCursor.hasNext()) {
            // We've exhausted the secondary's cursor already, so everything remaining from the
            // primary's cursor must be missing from secondary.
            docsMissingOnSecondary.push(primaryCursor.next());
        }

        while (secondaryCursor.hasNext()) {
            // We've exhausted the primary's cursor already, so everything remaining from the
            // secondary's cursor must be missing from primary.
            docsMissingOnPrimary.push(secondaryCursor.next());
        }

        return {docsWithDifferentContents, docsMissingOnPrimary, docsMissingOnSecondary};
    },
        "getHashes" : function(dbName, slaves) {
        assert.neq(dbName, 'local', 'Cannot run getHashes() on the "local" database');

        // _determineLiveSlaves() repopulates both 'self._slaves' and 'self._master'. If we're
        // passed an explicit set of slaves we don't want to do that.
        slaves = slaves || _determineLiveSlaves();

        const sessions = [
            this._master,
            ...slaves.filter(conn => {
                return !conn.adminCommand({isMaster: 1}).arbiterOnly;
            })
        ].map(conn => conn.getDB('test').getSession());

        // getHashes() is sometimes called for versions of MongoDB earlier than 4.0 so we cannot use
        // the dbHash command directly to filter out capped collections. checkReplicatedDataHashes()
        // uses the listCollections command after awaiting replication to determine if a collection
        // is capped.
        const hashes = this.getHashesUsingSessions(sessions, dbName, {filterCapped: false});
        return {master: hashes[0], slaves: hashes.slice(1)};
    },
        "findOplog" : function(conn, query, limit) {
        return conn.getDB('local')
            .getCollection(oplogName)
            .find(query)
            .sort({$natural: -1})
            .limit(limit);
    },
        "dumpOplog" : function(conn, query = {}, limit = 10) {
        var log = 'Dumping the latest ' + limit + ' documents that match ' + tojson(query) +
            ' from the oplog ' + oplogName + ' of ' + conn.host;
        let entries = [];
        let cursor = this.findOplog(conn, query, limit);
        cursor.forEach(function(entry) {
            log = log + '\n' + tojsononeline(entry);
            entries.push(entry);
        });
        jsTestLog(log);
        return entries;
    },
        "checkReplicaSet" : function(checkerFunction, slaves, ...checkerFunctionArgs) {
        assert.eq(typeof checkerFunction,
                  "function",
                  "Expected checkerFunction parameter to be a function");

        // Call getPrimary to populate rst with information about the nodes.
        var primary = this.getPrimary();
        assert(primary, 'calling getPrimary() failed');
        slaves = slaves || self._slaves;

        // Since we cannot determine if there is a background index in progress (SERVER-26624), we
        // use the "collMod" command to wait for any index builds that may be in progress on the
        // primary or on one of the secondaries to complete.
        for (let dbName of primary.getDBNames()) {
            if (dbName === "local") {
                continue;
            }

            let dbHandle = primary.getDB(dbName);
            dbHandle.getCollectionInfos({$or: [{type: "collection"}, {type: {$exists: false}}]})
                .forEach(function(collInfo) {
                    // Skip system collections. We handle here rather than in the getCollectionInfos
                    // filter to take advantage of the fact that a simple 'collection' filter will
                    // skip view evaluation, and therefore won't fail on an invalid view.
                    if (!collInfo.name.startsWith('system.')) {
                        // We intentionally await replication without doing any I/O to avoid any
                        // overhead. We call awaitReplication() later on to ensure the collMod
                        // is replicated to all nodes.
                        try {
                            assert.commandWorked(dbHandle.runCommand({collMod: collInfo.name}));
                        } catch (e) {
                            // Ignore NamespaceNotFound errors because a background thread could
                            // have dropped the collection after getCollectionInfos but before
                            // running collMod.
                            if (e.code != ErrorCodes.NamespaceNotFound) {
                                throw e;
                            }
                        }
                    }
                });
        }

        var activeException = false;

        // Lock the primary to prevent the TTL monitor from deleting expired documents in
        // the background while we are getting the dbhashes of the replica set members. It's not
        // important if the storage engine fails to perform its fsync operation. The only
        // requirement is that writes are locked out.
        assert.commandWorked(primary.adminCommand({fsync: 1, lock: 1, allowFsyncFailure: true}),
                             'failed to lock the primary');
        try {
            this.awaitReplication(null, null, slaves);
            checkerFunction.apply(this, checkerFunctionArgs);
        } catch (e) {
            activeException = true;
            throw e;
        } finally {
            // Allow writes on the primary.
            var res = primary.adminCommand({fsyncUnlock: 1});

            if (!res.ok) {
                var msg = 'failed to unlock the primary, which may cause this' +
                    ' test to hang: ' + tojson(res);
                if (activeException) {
                    print(msg);
                } else {
                    throw new Error(msg);
                }
            }
        }
    },
        "checkReplicatedDataHashes" : function(
        msgPrefix = 'checkReplicatedDataHashes', excludedDBs = [], ignoreUUIDs = false) {
        // Return items that are in either Array `a` or `b` but not both. Note that this will
        // not work with arrays containing NaN. Array.indexOf(NaN) will always return -1.

        var collectionPrinted = new Set();

        function arraySymmetricDifference(a, b) {
            var inAOnly = a.filter(function(elem) {
                return b.indexOf(elem) < 0;
            });

            var inBOnly = b.filter(function(elem) {
                return a.indexOf(elem) < 0;
            });

            return inAOnly.concat(inBOnly);
        }

        function checkDBHashesForReplSet(rst, dbBlacklist = [], slaves, msgPrefix, ignoreUUIDs) {
            // We don't expect the local database to match because some of its
            // collections are not replicated.
            dbBlacklist.push('local');
            slaves = slaves || rst._slaves;

            var success = true;
            var hasDumpedOplog = false;

            // Use '_master' instead of getPrimary() to avoid the detection of a new primary.
            // '_master' must have been populated.
            var primary = rst._master;
            var combinedDBs = new Set(primary.getDBNames());
            const replSetConfig = rst.getReplSetConfigFromNode();

            print("checkDBHashesForReplSet checking data hashes against primary: " + primary.host);

            slaves.forEach(node => {
                // Arbiters have no replicated data.
                if (isNodeArbiter(node)) {
                    print("checkDBHashesForReplSet skipping data of arbiter: " + node.host);
                    return;
                }
                print("checkDBHashesForReplSet going to check data hashes on secondary: " +
                      node.host);
                node.getDBNames().forEach(dbName => combinedDBs.add(dbName));
            });

            for (var dbName of combinedDBs) {
                if (Array.contains(dbBlacklist, dbName)) {
                    continue;
                }

                const dbHashes = rst.getHashes(dbName, slaves);
                const primaryDBHash = dbHashes.master;
                const primaryCollections = Object.keys(primaryDBHash.collections);
                assert.commandWorked(primaryDBHash);

                // Filter only collections that were retrieved by the dbhash. listCollections
                // may include non-replicated collections like system.profile.
                const primaryCollInfos = new CollInfos(primary, 'primary', dbName);
                primaryCollInfos.filter(primaryCollections);

                dbHashes.slaves.forEach(secondaryDBHash => {
                    assert.commandWorked(secondaryDBHash);

                    var secondary = secondaryDBHash._mongo;
                    var secondaryCollections = Object.keys(secondaryDBHash.collections);
                    // Check that collection information is consistent on the primary and
                    // secondaries.
                    const secondaryCollInfos = new CollInfos(secondary, 'secondary', dbName);
                    secondaryCollInfos.filter(secondaryCollections);

                    if (primaryCollections.length !== secondaryCollections.length) {
                        print(
                            msgPrefix +
                            ', the primary and secondary have a different number of collections: ' +
                            tojson(dbHashes));
                        for (var diffColl of arraySymmetricDifference(primaryCollections,
                                                                      secondaryCollections)) {
                            DataConsistencyChecker.dumpCollectionDiff(this,
                                                                      collectionPrinted,
                                                                      primaryCollInfos,
                                                                      secondaryCollInfos,
                                                                      diffColl);
                        }
                        success = false;
                    }

                    const nonCappedCollNames = primaryCollInfos.getNonCappedCollNames();
                    // Only compare the dbhashes of non-capped collections because capped
                    // collections are not necessarily truncated at the same points
                    // across replica set members.
                    nonCappedCollNames.forEach(collName => {
                        if (primaryDBHash.collections[collName] !==
                            secondaryDBHash.collections[collName]) {
                            print(msgPrefix +
                                  ', the primary and secondary have a different hash for the' +
                                  ' collection ' + dbName + '.' + collName + ': ' +
                                  tojson(dbHashes));
                            DataConsistencyChecker.dumpCollectionDiff(this,
                                                                      collectionPrinted,
                                                                      primaryCollInfos,
                                                                      secondaryCollInfos,
                                                                      collName);
                            success = false;
                        }
                    });

                    secondaryCollInfos.collInfosRes.forEach(secondaryInfo => {
                        primaryCollInfos.collInfosRes.forEach(primaryInfo => {
                            if (secondaryInfo.name === primaryInfo.name &&
                                secondaryInfo.type === primaryInfo.type) {
                                if (ignoreUUIDs) {
                                    print(msgPrefix + ", skipping UUID check for " +
                                          primaryInfo.name);
                                    primaryInfo.info.uuid = null;
                                    secondaryInfo.info.uuid = null;
                                }

                                // Ignore the 'flags' collection option as it was removed in 4.2
                                primaryInfo.options.flags = null;
                                secondaryInfo.options.flags = null;

                                if (!bsonBinaryEqual(secondaryInfo, primaryInfo)) {
                                    print(msgPrefix +
                                          ', the primary and secondary have different ' +
                                          'attributes for the collection or view ' + dbName + '.' +
                                          secondaryInfo.name);
                                    DataConsistencyChecker.dumpCollectionDiff(this,
                                                                              collectionPrinted,
                                                                              primaryCollInfos,
                                                                              secondaryCollInfos,
                                                                              secondaryInfo.name);
                                    success = false;
                                }
                            }
                        });
                    });

                    // Check that the following collection stats are the same across replica set
                    // members:
                    //  capped
                    //  nindexes, except on nodes with buildIndexes: false
                    //  ns
                    const hasSecondaryIndexes =
                        replSetConfig.members[rst.getNodeId(secondary)].buildIndexes !== false;
                    primaryCollections.forEach(collName => {
                        var primaryCollStats =
                            primary.getDB(dbName).runCommand({collStats: collName});
                        var secondaryCollStats =
                            secondary.getDB(dbName).runCommand({collStats: collName});

                        if (primaryCollStats.ok !== 1 || secondaryCollStats.ok !== 1) {
                            primaryCollInfos.print(collectionPrinted, collName);
                            secondaryCollInfos.print(collectionPrinted, collName);
                            success = false;
                        } else if (primaryCollStats.capped !== secondaryCollStats.capped ||
                                   (hasSecondaryIndexes &&
                                    primaryCollStats.nindexes !== secondaryCollStats.nindexes) ||
                                   primaryCollStats.ns !== secondaryCollStats.ns) {
                            print(msgPrefix +
                                  ', the primary and secondary have different stats for the ' +
                                  'collection ' + dbName + '.' + collName);
                            DataConsistencyChecker.dumpCollectionDiff(this,
                                                                      collectionPrinted,
                                                                      primaryCollInfos,
                                                                      secondaryCollInfos,
                                                                      collName);
                            success = false;
                        }
                    });

                    if (nonCappedCollNames.length === primaryCollections.length) {
                        // If the primary and secondary have the same hashes for all the
                        // collections in the database and there aren't any capped collections,
                        // then the hashes for the whole database should match.
                        if (primaryDBHash.md5 !== secondaryDBHash.md5) {
                            print(msgPrefix +
                                  ', the primary and secondary have a different hash for ' +
                                  'the ' + dbName + ' database: ' + tojson(dbHashes));
                            success = false;
                        }
                    }

                    if (!success) {
                        if (!hasDumpedOplog) {
                            print("checkDBHashesForReplSet dumping oplogs from all nodes");
                            this.dumpOplog(primary, {}, 100);
                            rst.getSecondaries().forEach(secondary =>
                                                             this.dumpOplog(secondary, {}, 100));
                            hasDumpedOplog = true;
                        }
                    }
                });
            }

            assert(success, 'dbhash mismatch between primary and secondary');
        }

        var liveSlaves = _determineLiveSlaves();
        this.checkReplicaSet(checkDBHashesForReplSet,
                             liveSlaves,
                             this,
                             excludedDBs,
                             liveSlaves,
                             msgPrefix,
                             ignoreUUIDs);
    },
        "checkOplogs" : function(msgPrefix) {
        var liveSlaves = _determineLiveSlaves();
        this.checkReplicaSet(checkOplogs, liveSlaves, this, liveSlaves, msgPrefix);
    },
        "checkCollectionCounts" : function(msgPrefix = 'checkCollectionCounts') {
        let success = true;
        const errPrefix = `${msgPrefix}, counts did not match for collection`;

        function checkCollectionCount(coll) {
            const itCount = coll.find().itcount();
            const fastCount = coll.count();
            if (itCount !== fastCount) {
                print(`${errPrefix} ${coll.getFullName()} on ${coll.getMongo().host}.` +
                      ` itcount: ${itCount}, fast count: ${fastCount}`);
                print("Collection info: " +
                      tojson(coll.getDB().getCollectionInfos({name: coll.getName()})));
                print("Collection stats: " + tojson(coll.stats()));
                print("First 10 documents in collection: " +
                      tojson(coll.find().limit(10).toArray()));

                // TODO (SERVER-35483): Remove this block and enable fastcount checks.
                if (coll.getFullName() == "config.transactions") {
                    print(`Ignoring fastcount error for ${coll.getFullName()} on ` +
                          `${coll.getMongo().host}. itcount: ${itCount}, fast count: ${fastCount}`);
                    return;
                }
                success = false;
            }
        }

        function checkCollectionCountsForDB(_db) {
            const res = assert.commandWorked(
                _db.runCommand({listCollections: 1, includePendingDrops: true}));
            const collNames = new DBCommandCursor(_db, res).toArray();
            collNames.forEach(c => checkCollectionCount(_db.getCollection(c.name)));
        }

        function checkCollectionCountsForNode(node) {
            const dbNames = node.getDBNames();
            dbNames.forEach(dbName => checkCollectionCountsForDB(node.getDB(dbName)));
        }

        function checkCollectionCountsForReplSet(rst) {
            rst.nodes.forEach(node => {
                // Arbiters have no replicated collections.
                if (isNodeArbiter(node)) {
                    print("checkCollectionCounts skipping counts for arbiter: " + node.host);
                    return;
                }
                checkCollectionCountsForNode(node);
            });
            assert(success, `Collection counts did not match. search for '${errPrefix}' in logs.`);
        }

        this.checkReplicaSet(checkCollectionCountsForReplSet, null, this);
    },
        "start" : function(n, options, restart, wait) {
        if (n.length) {
            var nodes = n;
            var started = [];

            for (var i = 0; i < nodes.length; i++) {
                if (this.start(nodes[i], Object.merge({}, options), restart, wait)) {
                    started.push(nodes[i]);
                }
            }

            return started;
        }

        // TODO: should we do something special if we don't currently know about this node?
        n = this.getNodeId(n);

        print("ReplSetTest n is : " + n);

        var defaults = {
            useHostName: this.useHostName,
            oplogSize: this.oplogSize,
            keyFile: this.keyFile,
            port: _useBridge ? _unbridgedPorts[n] : this.ports[n],
            replSet: this.useSeedList ? this.getURL() : this.name,
            dbpath: "$set-$node"
        };

        //
        // Note : this replaces the binVersion of the shared startSet() options the first time
        // through, so the full set is guaranteed to have different versions if size > 1.  If using
        // start() independently, independent version choices will be made
        //
        if (options && options.binVersion) {
            options.binVersion = MongoRunner.versionIterator(options.binVersion);
        }

        // If restarting a node, use its existing options as the defaults.
        var baseOptions;
        if ((options && options.restart) || restart) {
            baseOptions = _useBridge ? _unbridgedNodes[n].fullOptions : this.nodes[n].fullOptions;
        } else {
            baseOptions = defaults;
        }
        baseOptions = Object.merge(baseOptions, this.nodeOptions["n" + n]);
        options = Object.merge(baseOptions, options);
        if (options.hasOwnProperty("rsConfig")) {
            this.nodeOptions["n" + n] =
                Object.merge(this.nodeOptions["n" + n], {rsConfig: options.rsConfig});
        }
        delete options.rsConfig;

        options.restart = options.restart || restart;

        var pathOpts = {node: n, set: this.name};
        options.pathOpts = Object.merge(options.pathOpts || {}, pathOpts);

        // Turn off periodic noop writes for replica sets by default.
        options.setParameter = options.setParameter || {};
        if (typeof (options.setParameter) === "string") {
            var eqIdx = options.setParameter.indexOf("=");
            if (eqIdx != -1) {
                var param = options.setParameter.substring(0, eqIdx);
                var value = options.setParameter.substring(eqIdx + 1);
                options.setParameter = {};
                options.setParameter[param] = value;
            }
        }
        options.setParameter.writePeriodicNoops = options.setParameter.writePeriodicNoops || false;

        // We raise the number of initial sync connect attempts for tests that disallow chaining.
        // Disabling chaining can cause sync source selection to take longer so we must increase
        // the number of connection attempts.
        options.setParameter.numInitialSyncConnectAttempts =
            options.setParameter.numInitialSyncConnectAttempts || 60;

        if (tojson(options) != tojson({}))
            printjson(options);

        print("ReplSetTest " + (restart ? "(Re)" : "") + "Starting....");

        if (_useBridge && (restart === undefined || !restart)) {
            // We leave the mongobridge process running when the mongod process is restarted so we
            // don't need to start a new one.
            var bridgeOptions = Object.merge(_bridgeOptions, options.bridgeOptions || {});
            bridgeOptions = Object.merge(bridgeOptions, {
                hostName: this.host,
                port: this.ports[n],
                // The mongod processes identify themselves to mongobridge as host:port, where the
                // host is the actual hostname of the machine and not localhost.
                dest: getHostName() + ":" + _unbridgedPorts[n],
            });

            if (jsTestOptions().networkMessageCompressors) {
                bridgeOptions["networkMessageCompressors"] =
                    jsTestOptions().networkMessageCompressors;
            }

            this.nodes[n] = new MongoBridge(bridgeOptions);
        }

        var conn = MongoRunner.runMongod(options);
        if (!conn) {
            throw new Error("Failed to start node " + n);
        }

        // Make sure to call _addPath, otherwise folders won't be cleaned.
        this._addPath(conn.dbpath);

        if (_useBridge) {
            this.nodes[n].connectToBridge();
            _unbridgedNodes[n] = conn;
        } else {
            this.nodes[n] = conn;
        }

        // Add replica set specific attributes.
        this.nodes[n].nodeId = n;

        printjson(this.nodes);

        // Clean up after noReplSet to ensure it doesn't effect future restarts.
        if (options.noReplSet) {
            this.nodes[n].fullOptions.replSet = defaults.replSet;
            delete this.nodes[n].fullOptions.noReplSet;
        }

        wait = wait || false;
        if (!wait.toFixed) {
            if (wait)
                wait = 0;
            else
                wait = -1;
        }

        if (wait >= 0) {
            // Wait for node to start up.
            _waitForHealth(this.nodes[n], Health.UP, wait);
        }

        if (_causalConsistency) {
            this.nodes[n].setCausalConsistency(true);
        }

        return this.nodes[n];
    },
        "restart" : function(n, options, signal, wait) {
        // Can specify wait as third parameter, if using default signal
        if (signal == true || signal == false) {
            wait = signal;
            signal = undefined;
        }

        this.stop(n, signal, options, {forRestart: true});

        var started = this.start(n, options, true, wait);

        if (jsTestOptions().keyFile) {
            if (started.length) {
                // if n was an array of conns, start will return an array of connections
                for (var i = 0; i < started.length; i++) {
                    assert(jsTest.authenticate(started[i]), "Failed authentication during restart");
                }
            } else {
                assert(jsTest.authenticate(started), "Failed authentication during restart");
            }
        }
        return started;
    },
        "freeze" : function(node, ...wrappedArgs) {
            if (node.hasOwnProperty('length')) {
                let returnValueList = [];
                for (let i = 0; i < node.length; i++) {
                    returnValueList.push(wrapped.call(this, node[i], ...wrappedArgs));
                }

                return returnValueList;
            }

            return wrapped.call(this, node, ...wrappedArgs);
        },
        "stopMaster" : function(signal, opts) {
        var master = this.getPrimary();
        var master_id = this.getNodeId(master);
        return this.stop(master_id, signal, opts);
    },
        "stop" : function(n, signal, opts, {forRestart: forRestart = false} = {}) {
        // Flatten array of nodes to stop
        if (n.length) {
            var nodes = n;

            var stopped = [];
            for (var i = 0; i < nodes.length; i++) {
                if (this.stop(nodes[i], signal, opts))
                    stopped.push(nodes[i]);
            }

            return stopped;
        }

        // Can specify wait as second parameter, if using default signal
        if (signal == true || signal == false) {
            signal = undefined;
        }

        n = this.getNodeId(n);

        var conn = _useBridge ? _unbridgedNodes[n] : this.nodes[n];
        print('ReplSetTest stop *** Shutting down mongod in port ' + conn.port + ' ***');
        var ret = MongoRunner.stopMongod(conn, signal, opts);

        print('ReplSetTest stop *** Mongod in port ' + conn.port + ' shutdown with code (' + ret +
              ') ***');

        if (_useBridge && !forRestart) {
            // We leave the mongobridge process running when the mongod process is being restarted.
            const bridge = this.nodes[n];
            print('ReplSetTest stop *** Shutting down mongobridge on port ' + bridge.port + ' ***');
            const exitCode = bridge.stop();  // calls MongoBridge#stop()
            print('ReplSetTest stop *** mongobridge on port ' + bridge.port +
                  ' exited with code (' + exitCode + ') ***');
        }

        return ret;
    },
        "stopSet" : function(signal, forRestart, opts) {
        // Check to make sure data is the same on all nodes.
        if (!jsTest.options().skipCheckDBHashes) {
            print("ReplSetTest stopSet going to run data consistency checks.");
            // To skip this check add TestData.skipCheckDBHashes = true;
            // Reasons to skip this test include:
            // - the primary goes down and none can be elected (so fsync lock/unlock commands fail)
            // - the replica set is in an unrecoverable inconsistent state. E.g. the replica set
            //   is partitioned.
            //
            let master = _callIsMaster();
            if (master && this._liveNodes.length > 1) {  // skip for sets with 1 live node
                // Auth only on live nodes because authutil.assertAuthenticate
                // refuses to log in live connections if some secondaries are down.
                print("ReplSetTest stopSet checking oplogs.");
                asCluster(this._liveNodes, () => this.checkOplogs());
                print("ReplSetTest stopSet checking replicated data hashes.");
                asCluster(this._liveNodes, () => this.checkReplicatedDataHashes());
            } else {
                print(
                    "ReplSetTest stopSet skipped data consistency checks. Number of _liveNodes: " +
                    this._liveNodes.length + ", _callIsMaster response: " + master);
            }
            print("ReplSetTest stopSet finished data consistency checks.");
        }

        // Make shutdown faster in tests, especially when election handoff has no viable candidate.
        // Ignore errors from setParameter, perhaps it's a pre-4.1.10 mongod.
        if (_callIsMaster()) {
            asCluster(this._liveNodes, () => {
                for (let node of this._liveNodes) {
                    try {
                        print(
                            "ReplSetTest stopSet disabling 'waitForStepDownOnNonCommandShutdown' on " +
                            node.host);
                        assert.commandWorked(node.adminCommand({
                            setParameter: 1,
                            waitForStepDownOnNonCommandShutdown: false,
                        }));
                    } catch (e) {
                        print("Error in setParameter for waitForStepDownOnNonCommandShutdown:");
                        print(e);
                    }
                }
            });
        }

        print("ReplSetTest stopSet stopping all replica set nodes.");
        for (var i = 0; i < this.ports.length; i++) {
            this.stop(i, signal, opts);
        }
        print("ReplSetTest stopSet stopped all replica set nodes.");

        if (forRestart) {
            print("ReplSetTest stopSet returning since forRestart=true.");
            return;
        }

        if ((!opts || !opts.noCleanData) && _alldbpaths) {
            print("ReplSetTest stopSet deleting all dbpaths");
            for (var i = 0; i < _alldbpaths.length; i++) {
                print("ReplSetTest stopSet deleting dbpath: " + _alldbpaths[i]);
                resetDbpath(_alldbpaths[i]);
            }
            print("ReplSetTest stopSet deleted all dbpaths");
        }

        _forgetReplSet(this.name);

        print('ReplSetTest stopSet *** Shut down repl set - test worked ****');
    },
        "usesBridge" : function() {
        return _useBridge;
    },
        "waitForState" : function(node, state, timeout, reconnectNode) {
        _waitForIndicator(node, state, "state", timeout, reconnectNode);
    },
        "waitForMaster" : function(timeout) {
        var master;
        assert.soonNoExcept(function() {
            return (master = self.getPrimary());
        }, "waiting for master", timeout);

        return master;
    },
        "name" : "MiReplicaSet",
        "useHostName" : true,
        "host" : "DESKTOP-83BVJ92",
        "oplogSize" : 40,
        "useSeedList" : false,
        "keyFile" : undefined,
        "protocolVersion" : undefined,
        "waitForKeys" : undefined,
        "nodeOptions" : {
                "n0" : undefined,
                "n1" : undefined,
                "n2" : undefined
        },
        "nodes" : [ ],
        "ports" : [
                20000,
                20001,
                20002
        ]
}
> ejemploReplcaSet.startSet()
2024-05-25T09:29:01.142-0500 E  QUERY    [js] uncaught exception: ReferenceError: ejemploReplcaSet is not defined :
@(shell):1:1
> ejemploReplicaSet.startSet()
ReplSetTest starting set
ReplSetTest n is : 0
{
        "useHostName" : true,
        "oplogSize" : 40,
        "keyFile" : undefined,
        "port" : 20000,
        "replSet" : "MiReplicaSet",
        "dbpath" : "$set-$node",
        "restart" : undefined,
        "pathOpts" : {
                "node" : 0,
                "set" : "MiReplicaSet"
        },
        "setParameter" : {
                "writePeriodicNoops" : false,
                "numInitialSyncConnectAttempts" : 60
        }
}
ReplSetTest Starting....
Resetting db path '/data/db/MiReplicaSet-0'
2024-05-25T09:29:31.332-0500 I  -        [js] shell: started program (sh11176):  C:\Program Files\MongoDB\Server\4.2\bin\mongod.exe --oplogSize 40 --port 20000 --replSet MiReplicaSet --dbpath /data/db/MiReplicaSet-0 --setParameter writePeriodicNoops=false --setParameter numInitialSyncConnectAttempts=60 --bind_ip 0.0.0.0 --setParameter enableTestCommands=1 --setParameter disableLogicalSessionCacheRefresh=true --setParameter minNumChunksForSessionsCollection=1 --setParameter orphanCleanupDelaySecs=1
d20000| 2024-05-25T09:29:31.366-0500 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
d20000| 2024-05-25T09:29:31.853-0500 W  ASIO     [main] No TransportLayer configured during NetworkInterface startup
d20000| 2024-05-25T09:29:31.855-0500 I  CONTROL  [initandlisten] MongoDB starting : pid=11176 port=20000 dbpath=/data/db/MiReplicaSet-0 64-bit host=DESKTOP-83BVJ92
d20000| 2024-05-25T09:29:31.855-0500 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
d20000| 2024-05-25T09:29:31.855-0500 I  CONTROL  [initandlisten] db version v4.2.25
d20000| 2024-05-25T09:29:31.855-0500 I  CONTROL  [initandlisten] git version: 41b59c2bfb5121e66f18cc3ef40055a1b5fb6c2e
d20000| 2024-05-25T09:29:31.855-0500 I  CONTROL  [initandlisten] allocator: tcmalloc
d20000| 2024-05-25T09:29:31.855-0500 I  CONTROL  [initandlisten] modules: none
d20000| 2024-05-25T09:29:31.855-0500 I  CONTROL  [initandlisten] build environment:
d20000| 2024-05-25T09:29:31.855-0500 I  CONTROL  [initandlisten]     distmod: 2012plus
d20000| 2024-05-25T09:29:31.855-0500 I  CONTROL  [initandlisten]     distarch: x86_64
d20000| 2024-05-25T09:29:31.855-0500 I  CONTROL  [initandlisten]     target_arch: x86_64
d20000| 2024-05-25T09:29:31.855-0500 I  CONTROL  [initandlisten] options: { net: { bindIp: "0.0.0.0", port: 20000 }, replication: { oplogSizeMB: 40, replSet: "MiReplicaSet" }, setParameter: { disableLogicalSessionCacheRefresh: "true", enableTestCommands: "1", minNumChunksForSessionsCollection: "1", numInitialSyncConnectAttempts: "60", orphanCleanupDelaySecs: "1", writePeriodicNoops: "false" }, storage: { dbPath: "/data/db/MiReplicaSet-0" } }
d20000| 2024-05-25T09:29:31.865-0500 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=3520M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000,close_scan_interval=10,close_handle_minimum=250),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
d20000| 2024-05-25T09:29:32.056-0500 I  STORAGE  [initandlisten] WiredTiger message [1716647372:55898][11176:140709214902368], txn-recover: Set global recovery timestamp: (0, 0)
d20000| 2024-05-25T09:29:32.176-0500 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(0, 0)
d20000| 2024-05-25T09:29:32.325-0500 I  STORAGE  [initandlisten] Timestamp monitor starting
d20000| 2024-05-25T09:29:32.430-0500 I  CONTROL  [initandlisten]
d20000| 2024-05-25T09:29:32.430-0500 I  CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
d20000| 2024-05-25T09:29:32.430-0500 I  CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
d20000| 2024-05-25T09:29:32.430-0500 I  CONTROL  [initandlisten]
d20000| 2024-05-25T09:29:32.434-0500 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
d20000| 2024-05-25T09:29:32.448-0500 I  STORAGE  [initandlisten] createCollection: local.startup_log with generated UUID: 0e1e4fe5-d078-4374-9cab-3ed066824159 and options: { capped: true, size: 10485760 }
d20000| 2024-05-25T09:29:32.588-0500 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.startup_log
d20000| 2024-05-25T09:29:34.194-0500 W  FTDC     [initandlisten] Failed to initialize Performance Counters for FTDC: WindowsPdhError: PdhExpandCounterPathW failed with 'El objeto especificado no se encontr├│ en el equipo.' for counter '\Processor(_Total)\% Idle Time'
d20000| 2024-05-25T09:29:34.195-0500 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/data/db/MiReplicaSet-0/diagnostic.data'
d20000| 2024-05-25T09:29:34.280-0500 I  STORAGE  [initandlisten] createCollection: local.replset.oplogTruncateAfterPoint with generated UUID: 954ce413-9ef3-4d45-b1df-4ea7c845a021 and options: {}
d20000| 2024-05-25T09:29:34.399-0500 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.oplogTruncateAfterPoint
d20000| 2024-05-25T09:29:34.399-0500 I  STORAGE  [initandlisten] createCollection: local.replset.minvalid with generated UUID: 99e24be6-10fc-40b1-89e0-7c470e19d218 and options: {}
d20000| 2024-05-25T09:29:34.496-0500 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.minvalid
d20000| 2024-05-25T09:29:34.509-0500 I  STORAGE  [initandlisten] createCollection: local.replset.election with generated UUID: 985a3309-c26f-4834-a8e3-976f02feb8fa and options: {}
d20000| 2024-05-25T09:29:34.613-0500 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.election
d20000| 2024-05-25T09:29:34.625-0500 I  REPL     [initandlisten] Did not find local initialized voted for document at startup.
d20000| 2024-05-25T09:29:34.625-0500 I  REPL     [initandlisten] Did not find local Rollback ID document at startup. Creating one.
d20000| 2024-05-25T09:29:34.625-0500 I  STORAGE  [initandlisten] createCollection: local.system.rollback.id with generated UUID: bcc3092f-7715-439f-8e01-ce102e0d4249 and options: {}
d20000| 2024-05-25T09:29:34.721-0500 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.system.rollback.id
d20000| 2024-05-25T09:29:34.721-0500 I  REPL     [initandlisten] Initialized the rollback ID to 1
d20000| 2024-05-25T09:29:34.722-0500 I  REPL     [initandlisten] Did not find local replica set configuration document at startup;  NoMatchingDocument: Did not find replica set configuration document in local.system.replset
d20000| 2024-05-25T09:29:34.727-0500 I  NETWORK  [listener] Listening on 0.0.0.0
d20000| 2024-05-25T09:29:34.727-0500 I  NETWORK  [listener] waiting for connections on port 20000
d20000| 2024-05-25T09:29:35.205-0500 I  NETWORK  [listener] connection accepted from 127.0.0.1:55134 #1 (1 connection now open)
d20000| 2024-05-25T09:29:35.214-0500 I  NETWORK  [conn1] received client metadata from 127.0.0.1:55134 conn1: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
[ connection to DESKTOP-83BVJ92:20000 ]
ReplSetTest n is : 1
{
        "useHostName" : true,
        "oplogSize" : 40,
        "keyFile" : undefined,
        "port" : 20001,
        "replSet" : "MiReplicaSet",
        "dbpath" : "$set-$node",
        "restart" : undefined,
        "pathOpts" : {
                "node" : 1,
                "set" : "MiReplicaSet"
        },
        "setParameter" : {
                "writePeriodicNoops" : false,
                "numInitialSyncConnectAttempts" : 60
        }
}
ReplSetTest Starting....
Resetting db path '/data/db/MiReplicaSet-1'
2024-05-25T09:29:35.241-0500 I  -        [js] shell: started program (sh9072):  C:\Program Files\MongoDB\Server\4.2\bin\mongod.exe --oplogSize 40 --port 20001 --replSet MiReplicaSet --dbpath /data/db/MiReplicaSet-1 --setParameter writePeriodicNoops=false --setParameter numInitialSyncConnectAttempts=60 --bind_ip 0.0.0.0 --setParameter enableTestCommands=1 --setParameter disableLogicalSessionCacheRefresh=true --setParameter minNumChunksForSessionsCollection=1 --setParameter orphanCleanupDelaySecs=1
d20001| 2024-05-25T09:29:35.284-0500 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
d20001| 2024-05-25T09:29:35.773-0500 W  ASIO     [main] No TransportLayer configured during NetworkInterface startup
d20001| 2024-05-25T09:29:35.776-0500 I  CONTROL  [initandlisten] MongoDB starting : pid=9072 port=20001 dbpath=/data/db/MiReplicaSet-1 64-bit host=DESKTOP-83BVJ92
d20001| 2024-05-25T09:29:35.776-0500 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
d20001| 2024-05-25T09:29:35.776-0500 I  CONTROL  [initandlisten] db version v4.2.25
d20001| 2024-05-25T09:29:35.776-0500 I  CONTROL  [initandlisten] git version: 41b59c2bfb5121e66f18cc3ef40055a1b5fb6c2e
d20001| 2024-05-25T09:29:35.776-0500 I  CONTROL  [initandlisten] allocator: tcmalloc
d20001| 2024-05-25T09:29:35.776-0500 I  CONTROL  [initandlisten] modules: none
d20001| 2024-05-25T09:29:35.776-0500 I  CONTROL  [initandlisten] build environment:
d20001| 2024-05-25T09:29:35.776-0500 I  CONTROL  [initandlisten]     distmod: 2012plus
d20001| 2024-05-25T09:29:35.776-0500 I  CONTROL  [initandlisten]     distarch: x86_64
d20001| 2024-05-25T09:29:35.776-0500 I  CONTROL  [initandlisten]     target_arch: x86_64
d20001| 2024-05-25T09:29:35.776-0500 I  CONTROL  [initandlisten] options: { net: { bindIp: "0.0.0.0", port: 20001 }, replication: { oplogSizeMB: 40, replSet: "MiReplicaSet" }, setParameter: { disableLogicalSessionCacheRefresh: "true", enableTestCommands: "1", minNumChunksForSessionsCollection: "1", numInitialSyncConnectAttempts: "60", orphanCleanupDelaySecs: "1", writePeriodicNoops: "false" }, storage: { dbPath: "/data/db/MiReplicaSet-1" } }
d20001| 2024-05-25T09:29:35.779-0500 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=3520M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000,close_scan_interval=10,close_handle_minimum=250),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
d20001| 2024-05-25T09:29:35.964-0500 I  STORAGE  [initandlisten] WiredTiger message [1716647375:963355][9072:140709214902368], txn-recover: Set global recovery timestamp: (0, 0)
d20001| 2024-05-25T09:29:36.114-0500 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(0, 0)
d20001| 2024-05-25T09:29:36.275-0500 I  STORAGE  [initandlisten] Timestamp monitor starting
d20001| 2024-05-25T09:29:36.308-0500 I  CONTROL  [initandlisten]
d20001| 2024-05-25T09:29:36.308-0500 I  CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
d20001| 2024-05-25T09:29:36.308-0500 I  CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
d20001| 2024-05-25T09:29:36.309-0500 I  CONTROL  [initandlisten]
d20001| 2024-05-25T09:29:36.324-0500 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
d20001| 2024-05-25T09:29:36.325-0500 I  STORAGE  [initandlisten] createCollection: local.startup_log with generated UUID: f358ccd4-6d62-43a0-a70b-a3b12fac5bee and options: { capped: true, size: 10485760 }
d20001| 2024-05-25T09:29:36.482-0500 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.startup_log
d20001| 2024-05-25T09:29:36.954-0500 W  FTDC     [initandlisten] Failed to initialize Performance Counters for FTDC: WindowsPdhError: PdhExpandCounterPathW failed with 'El objeto especificado no se encontr├│ en el equipo.' for counter '\Memory\Available Bytes'
d20001| 2024-05-25T09:29:36.954-0500 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/data/db/MiReplicaSet-1/diagnostic.data'
d20001| 2024-05-25T09:29:36.962-0500 I  STORAGE  [initandlisten] createCollection: local.replset.oplogTruncateAfterPoint with generated UUID: ef66bbe0-0d95-4527-8138-b4d5bb36b659 and options: {}
d20001| 2024-05-25T09:29:37.074-0500 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.oplogTruncateAfterPoint
d20001| 2024-05-25T09:29:37.075-0500 W  REPL     [ftdc] Rollback ID is not initialized yet.
d20001| 2024-05-25T09:29:37.075-0500 I  STORAGE  [initandlisten] createCollection: local.replset.minvalid with generated UUID: 41dbfd86-0c42-4a21-9368-134d3fb1e9d0 and options: {}
d20001| 2024-05-25T09:29:37.434-0500 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.minvalid
d20001| 2024-05-25T09:29:37.434-0500 I  STORAGE  [initandlisten] createCollection: local.replset.election with generated UUID: 6c2d845c-bee0-4097-841a-8c7d620d40b2 and options: {}
d20001| 2024-05-25T09:29:37.775-0500 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.election
d20001| 2024-05-25T09:29:37.775-0500 I  REPL     [initandlisten] Did not find local initialized voted for document at startup.
d20001| 2024-05-25T09:29:37.775-0500 I  REPL     [initandlisten] Did not find local Rollback ID document at startup. Creating one.
d20001| 2024-05-25T09:29:37.775-0500 I  STORAGE  [initandlisten] createCollection: local.system.rollback.id with generated UUID: 18ba3e02-0de5-4633-a5d8-defef8541603 and options: {}
d20001| 2024-05-25T09:29:37.875-0500 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.system.rollback.id
d20001| 2024-05-25T09:29:37.875-0500 I  REPL     [initandlisten] Initialized the rollback ID to 1
d20001| 2024-05-25T09:29:37.875-0500 I  REPL     [initandlisten] Did not find local replica set configuration document at startup;  NoMatchingDocument: Did not find replica set configuration document in local.system.replset
d20001| 2024-05-25T09:29:37.880-0500 I  NETWORK  [listener] Listening on 0.0.0.0
d20001| 2024-05-25T09:29:37.880-0500 I  NETWORK  [listener] waiting for connections on port 20001
d20001| 2024-05-25T09:29:37.994-0500 I  NETWORK  [listener] connection accepted from 127.0.0.1:55140 #1 (1 connection now open)
d20001| 2024-05-25T09:29:38.004-0500 I  NETWORK  [conn1] received client metadata from 127.0.0.1:55140 conn1: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
[ connection to DESKTOP-83BVJ92:20000, connection to DESKTOP-83BVJ92:20001 ]
ReplSetTest n is : 2
{
        "useHostName" : true,
        "oplogSize" : 40,
        "keyFile" : undefined,
        "port" : 20002,
        "replSet" : "MiReplicaSet",
        "dbpath" : "$set-$node",
        "restart" : undefined,
        "pathOpts" : {
                "node" : 2,
                "set" : "MiReplicaSet"
        },
        "setParameter" : {
                "writePeriodicNoops" : false,
                "numInitialSyncConnectAttempts" : 60
        }
}
ReplSetTest Starting....
Resetting db path '/data/db/MiReplicaSet-2'
2024-05-25T09:29:38.014-0500 I  -        [js] shell: started program (sh10880):  C:\Program Files\MongoDB\Server\4.2\bin\mongod.exe --oplogSize 40 --port 20002 --replSet MiReplicaSet --dbpath /data/db/MiReplicaSet-2 --setParameter writePeriodicNoops=false --setParameter numInitialSyncConnectAttempts=60 --bind_ip 0.0.0.0 --setParameter enableTestCommands=1 --setParameter disableLogicalSessionCacheRefresh=true --setParameter minNumChunksForSessionsCollection=1 --setParameter orphanCleanupDelaySecs=1
d20002| 2024-05-25T09:29:38.122-0500 I  CONTROL  [main] Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'
d20002| 2024-05-25T09:29:38.612-0500 W  ASIO     [main] No TransportLayer configured during NetworkInterface startup
d20002| 2024-05-25T09:29:38.614-0500 I  CONTROL  [initandlisten] MongoDB starting : pid=10880 port=20002 dbpath=/data/db/MiReplicaSet-2 64-bit host=DESKTOP-83BVJ92
d20002| 2024-05-25T09:29:38.614-0500 I  CONTROL  [initandlisten] targetMinOS: Windows 7/Windows Server 2008 R2
d20002| 2024-05-25T09:29:38.614-0500 I  CONTROL  [initandlisten] db version v4.2.25
d20002| 2024-05-25T09:29:38.615-0500 I  CONTROL  [initandlisten] git version: 41b59c2bfb5121e66f18cc3ef40055a1b5fb6c2e
d20002| 2024-05-25T09:29:38.615-0500 I  CONTROL  [initandlisten] allocator: tcmalloc
d20002| 2024-05-25T09:29:38.615-0500 I  CONTROL  [initandlisten] modules: none
d20002| 2024-05-25T09:29:38.615-0500 I  CONTROL  [initandlisten] build environment:
d20002| 2024-05-25T09:29:38.615-0500 I  CONTROL  [initandlisten]     distmod: 2012plus
d20002| 2024-05-25T09:29:38.615-0500 I  CONTROL  [initandlisten]     distarch: x86_64
d20002| 2024-05-25T09:29:38.615-0500 I  CONTROL  [initandlisten]     target_arch: x86_64
d20002| 2024-05-25T09:29:38.615-0500 I  CONTROL  [initandlisten] options: { net: { bindIp: "0.0.0.0", port: 20002 }, replication: { oplogSizeMB: 40, replSet: "MiReplicaSet" }, setParameter: { disableLogicalSessionCacheRefresh: "true", enableTestCommands: "1", minNumChunksForSessionsCollection: "1", numInitialSyncConnectAttempts: "60", orphanCleanupDelaySecs: "1", writePeriodicNoops: "false" }, storage: { dbPath: "/data/db/MiReplicaSet-2" } }
d20002| 2024-05-25T09:29:38.617-0500 I  STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=3520M,cache_overflow=(file_max=0M),session_max=33000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000,close_scan_interval=10,close_handle_minimum=250),statistics_log=(wait=0),verbose=[recovery_progress,checkpoint_progress],
d20002| 2024-05-25T09:29:38.823-0500 I  STORAGE  [initandlisten] WiredTiger message [1716647378:822876][10880:140709214902368], txn-recover: Set global recovery timestamp: (0, 0)
d20002| 2024-05-25T09:29:39.007-0500 I  RECOVERY [initandlisten] WiredTiger recoveryTimestamp. Ts: Timestamp(0, 0)
d20002| 2024-05-25T09:29:39.164-0500 I  STORAGE  [initandlisten] Timestamp monitor starting
d20002| 2024-05-25T09:29:39.284-0500 I  CONTROL  [initandlisten]
d20002| 2024-05-25T09:29:39.284-0500 I  CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
d20002| 2024-05-25T09:29:39.284-0500 I  CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
d20002| 2024-05-25T09:29:39.285-0500 I  CONTROL  [initandlisten]
d20002| 2024-05-25T09:29:39.296-0500 I  STORAGE  [initandlisten] Flow Control is enabled on this deployment.
d20002| 2024-05-25T09:29:39.300-0500 I  STORAGE  [initandlisten] createCollection: local.startup_log with generated UUID: 68771ea7-7ec7-477d-a2df-b4e2fa4b24e8 and options: { capped: true, size: 10485760 }
d20002| 2024-05-25T09:29:39.433-0500 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.startup_log
d20002| 2024-05-25T09:29:39.880-0500 W  FTDC     [initandlisten] Failed to initialize Performance Counters for FTDC: WindowsPdhError: PdhExpandCounterPathW failed with 'El objeto especificado no se encontr├│ en el equipo.' for counter '\Memory\Available Bytes'
d20002| 2024-05-25T09:29:39.880-0500 I  FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/data/db/MiReplicaSet-2/diagnostic.data'
d20002| 2024-05-25T09:29:39.885-0500 I  STORAGE  [initandlisten] createCollection: local.replset.oplogTruncateAfterPoint with generated UUID: 2e5ad36e-7178-460f-9112-fdc7614f3935 and options: {}
d20002| 2024-05-25T09:29:40.046-0500 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.oplogTruncateAfterPoint
d20002| 2024-05-25T09:29:40.047-0500 W  REPL     [ftdc] Rollback ID is not initialized yet.
d20002| 2024-05-25T09:29:40.047-0500 I  STORAGE  [initandlisten] createCollection: local.replset.minvalid with generated UUID: b33df84b-021c-41c3-b0fc-5a17782068b1 and options: {}
d20002| 2024-05-25T09:29:40.180-0500 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.minvalid
d20002| 2024-05-25T09:29:40.180-0500 I  STORAGE  [initandlisten] createCollection: local.replset.election with generated UUID: 623eef86-4032-41a2-8724-ef905f5b0a74 and options: {}
d20002| 2024-05-25T09:29:40.322-0500 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.replset.election
d20002| 2024-05-25T09:29:40.323-0500 I  REPL     [initandlisten] Did not find local initialized voted for document at startup.
d20002| 2024-05-25T09:29:40.323-0500 I  REPL     [initandlisten] Did not find local Rollback ID document at startup. Creating one.
d20002| 2024-05-25T09:29:40.323-0500 I  STORAGE  [initandlisten] createCollection: local.system.rollback.id with generated UUID: e1b0b301-ade4-43d4-bcfd-d74fe795fe95 and options: {}
d20002| 2024-05-25T09:29:40.441-0500 I  INDEX    [initandlisten] index build: done building index _id_ on ns local.system.rollback.id
d20002| 2024-05-25T09:29:40.442-0500 I  REPL     [initandlisten] Initialized the rollback ID to 1
d20002| 2024-05-25T09:29:40.442-0500 I  REPL     [initandlisten] Did not find local replica set configuration document at startup;  NoMatchingDocument: Did not find replica set configuration document in local.system.replset
d20002| 2024-05-25T09:29:40.453-0500 I  NETWORK  [listener] Listening on 0.0.0.0
d20002| 2024-05-25T09:29:40.453-0500 I  NETWORK  [listener] waiting for connections on port 20002
d20002| 2024-05-25T09:29:40.755-0500 I  NETWORK  [listener] connection accepted from 127.0.0.1:55144 #1 (1 connection now open)
d20002| 2024-05-25T09:29:40.765-0500 I  NETWORK  [conn1] received client metadata from 127.0.0.1:55144 conn1: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
[
        connection to DESKTOP-83BVJ92:20000,
        connection to DESKTOP-83BVJ92:20001,
        connection to DESKTOP-83BVJ92:20002
]
[
        connection to DESKTOP-83BVJ92:20000,
        connection to DESKTOP-83BVJ92:20001,
        connection to DESKTOP-83BVJ92:20002
]
> ejemploReplicaSet.initiate()
{
        "replSetInitiate" : {
                "_id" : "MiReplicaSet",
                "protocolVersion" : 1,
                "members" : [
                        {
                                "_id" : 0,
                                "host" : "DESKTOP-83BVJ92:20000"
                        }
                ]
        }
}
d20000| 2024-05-25T09:31:23.231-0500 I  REPL     [conn1] replSetInitiate admin command received from client
d20000| 2024-05-25T09:31:23.268-0500 I  REPL     [conn1] replSetInitiate config object with 1 members parses ok
d20000| 2024-05-25T09:31:23.298-0500 I  REPL     [conn1] ******
d20000| 2024-05-25T09:31:23.298-0500 I  REPL     [conn1] creating replication oplog of size: 40MB...
d20000| 2024-05-25T09:31:23.298-0500 I  STORAGE  [conn1] createCollection: local.oplog.rs with generated UUID: 4d09b516-3425-4cd2-bbbd-e925e918c2a0 and options: { capped: true, size: 41943040, autoIndexId: false }
d20000| 2024-05-25T09:31:23.345-0500 I  STORAGE  [conn1] Starting OplogTruncaterThread local.oplog.rs
d20000| 2024-05-25T09:31:23.346-0500 I  STORAGE  [conn1] The size storer reports that the oplog contains 0 records totaling to 0 bytes
d20000| 2024-05-25T09:31:23.346-0500 I  STORAGE  [conn1] Scanning the oplog to determine where to place markers for truncation
d20000| 2024-05-25T09:31:23.346-0500 I  STORAGE  [conn1] WiredTiger record store oplog processing took 0ms
d20000| 2024-05-25T09:31:23.500-0500 I  REPL     [conn1] ******
d20000| 2024-05-25T09:31:23.501-0500 I  STORAGE  [conn1] createCollection: local.system.replset with generated UUID: 89a9b04b-9665-4036-9f99-e399327a7682 and options: {}
d20000| 2024-05-25T09:31:23.612-0500 I  INDEX    [conn1] index build: done building index _id_ on ns local.system.replset
d20000| 2024-05-25T09:31:23.641-0500 I  COMMAND  [conn1] CMD: collMod: { collMod: "startup_log" }
d20000| 2024-05-25T09:31:23.647-0500 I  COMMAND  [conn1] CMD: collMod: { collMod: "oplog.rs" }
d20000| 2024-05-25T09:31:23.647-0500 I  COMMAND  [conn1] CMD: collMod: { collMod: "system.replset" }
d20000| 2024-05-25T09:31:23.647-0500 I  COMMAND  [conn1] CMD: collMod: { collMod: "replset.oplogTruncateAfterPoint" }
d20000| 2024-05-25T09:31:23.647-0500 I  COMMAND  [conn1] CMD: collMod: { collMod: "replset.election" }
d20000| 2024-05-25T09:31:23.647-0500 I  COMMAND  [conn1] CMD: collMod: { collMod: "replset.minvalid" }
d20000| 2024-05-25T09:31:23.647-0500 I  COMMAND  [conn1] CMD: collMod: { collMod: "system.rollback.id" }
d20000| 2024-05-25T09:31:23.647-0500 I  STORAGE  [conn1] createCollection: admin.system.version with provided UUID: be3bfec4-3482-4616-a773-7a4e178c6a48 and options: { uuid: UUID("be3bfec4-3482-4616-a773-7a4e178c6a48") }
d20000| 2024-05-25T09:31:23.779-0500 I  INDEX    [conn1] index build: done building index _id_ on ns admin.system.version
d20000| 2024-05-25T09:31:23.779-0500 I  COMMAND  [conn1] setting featureCompatibilityVersion to 4.2
d20000| 2024-05-25T09:31:23.779-0500 I  NETWORK  [conn1] Skip closing connection for connection # 1
d20000| 2024-05-25T09:31:23.796-0500 I  REPL     [conn1] New replica set config in use: { _id: "MiReplicaSet", version: 1, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "DESKTOP-83BVJ92:20000", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('6651f63be8c43419705d492c') } }
d20000| 2024-05-25T09:31:23.796-0500 I  REPL     [conn1] This node is DESKTOP-83BVJ92:20000 in the config
d20000| 2024-05-25T09:31:23.796-0500 I  REPL     [conn1] transition to STARTUP2 from STARTUP
d20000| 2024-05-25T09:31:23.797-0500 I  REPL     [conn1] Starting replication storage threads
d20000| 2024-05-25T09:31:23.800-0500 I  REPL     [conn1] transition to RECOVERING from STARTUP2
d20000| 2024-05-25T09:31:23.801-0500 I  REPL     [conn1] Starting replication fetcher thread
d20000| 2024-05-25T09:31:23.801-0500 I  REPL     [conn1] Starting replication applier thread
d20000| 2024-05-25T09:31:23.802-0500 I  REPL     [conn1] Starting replication reporter thread
d20000| 2024-05-25T09:31:23.802-0500 I  REPL     [rsSync-0] Starting oplog application
d20000| 2024-05-25T09:31:23.803-0500 I  COMMAND  [conn1] command local.system.rollback.id appName: "MongoDB Shell" command: replSetInitiate { replSetInitiate: { _id: "MiReplicaSet", protocolVersion: 1.0, members: [ { _id: 0.0, host: "DESKTOP-83BVJ92:20000" } ] }, lsid: { id: UUID("c0238a8b-ad5d-42ed-b9ef-241f8e19144c") }, $db: "admin" } numYields:0 reslen:38 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 19 } }, ReplicationStateTransition: { acquireCount: { w: 19 } }, Global: { acquireCount: { r: 4, w: 13, W: 2 }, acquireWaitCount: { W: 1 }, timeAcquiringMicros: { W: 137 } }, Database: { acquireCount: { r: 2, w: 9, W: 4 } }, Collection: { acquireCount: { r: 2, w: 2, W: 13 } }, Mutex: { acquireCount: { r: 16 } }, oplog: { acquireCount: { r: 1, w: 1, W: 1 } } } flowControl:{ acquireCount: 4, timeAcquiringMicros: 16 } storage:{} protocol:op_msg 587ms
d20000| 2024-05-25T09:31:23.807-0500 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
d20000| 2024-05-25T09:31:23.852-0500 I  ELECTION [rsSync-0] conducting a dry run election to see if we could be elected. current term: 0
d20000| 2024-05-25T09:31:23.854-0500 I  ELECTION [replexec-0] dry election run succeeded, running for election in term 1
d20000| 2024-05-25T09:31:23.894-0500 I  ELECTION [replexec-0] election succeeded, assuming primary role in term 1
d20000| 2024-05-25T09:31:23.894-0500 I  REPL     [replexec-0] transition to PRIMARY from SECONDARY
d20000| 2024-05-25T09:31:23.894-0500 I  REPL     [replexec-0] Resetting sync source to empty, which was :27017
d20000| 2024-05-25T09:31:23.894-0500 I  REPL     [replexec-0] Entering primary catch-up mode.
d20000| 2024-05-25T09:31:23.895-0500 I  REPL     [replexec-0] Exited primary catch-up mode.
d20000| 2024-05-25T09:31:23.895-0500 I  REPL     [replexec-0] Stopping replication producer
d20000| 2024-05-25T09:31:23.895-0500 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 1
d20000| 2024-05-25T09:31:23.895-0500 I  REPL     [ReplBatcher] Oplog buffer has been drained in term 1
d20000| 2024-05-25T09:31:23.895-0500 I  REPL     [RstlKillOpThread] Starting to kill user operations
d20000| 2024-05-25T09:31:23.896-0500 I  REPL     [RstlKillOpThread] Stopped killing user operations
d20000| 2024-05-25T09:31:23.896-0500 I  REPL     [RstlKillOpThread] State transition ops metrics: { lastStateTransition: "stepUp", userOpsKilled: 0, userOpsRunning: 0 }
d20000| 2024-05-25T09:31:23.896-0500 I  STORAGE  [rsSync-0] createCollection: config.transactions with generated UUID: cde5c6f4-bbd6-4735-bcc0-16f6fa676b79 and options: {}
d20000| 2024-05-25T09:31:24.071-0500 I  INDEX    [rsSync-0] index build: done building index _id_ on ns config.transactions
d20000| 2024-05-25T09:31:24.071-0500 I  STORAGE  [rsSync-0] createCollection: config.image_collection with generated UUID: b444e2ad-3f31-4d9a-81cc-fe05abfc605e and options: {}
d20000| 2024-05-25T09:31:24.209-0500 I  INDEX    [rsSync-0] index build: done building index _id_ on ns config.image_collection
d20000| 2024-05-25T09:31:24.215-0500 I  REPL     [rsSync-0] transition to primary complete; database writes are now permitted
d20000| 2024-05-25T09:31:24.217-0500 I  STORAGE  [monitoring-keys-for-HMAC] createCollection: admin.system.keys with generated UUID: 192a8fff-1fcb-4b7e-a8d7-ba8a7902c5c4 and options: {}
d20000| 2024-05-25T09:31:24.300-0500 I  STORAGE  [WTJournalFlusher] Triggering the first stable checkpoint. Initial Data: Timestamp(1716647483, 1) PrevStable: Timestamp(0, 0) CurrStable: Timestamp(1716647484, 2)
Reconfiguring replica set to add in other nodes
{
        "replSetReconfig" : {
                "_id" : "MiReplicaSet",
                "protocolVersion" : 1,
                "members" : [
                        {
                                "_id" : 0,
                                "host" : "DESKTOP-83BVJ92:20000"
                        },
                        {
                                "_id" : 1,
                                "host" : "DESKTOP-83BVJ92:20001"
                        },
                        {
                                "_id" : 2,
                                "host" : "DESKTOP-83BVJ92:20002"
                        }
                ],
                "version" : 2
        }
}
d20000| 2024-05-25T09:31:24.302-0500 I  REPL     [conn1] replSetReconfig admin command received from client; new config: { _id: "MiReplicaSet", protocolVersion: 1.0, members: [ { _id: 0.0, host: "DESKTOP-83BVJ92:20000" }, { _id: 1.0, host: "DESKTOP-83BVJ92:20001" }, { _id: 2.0, host: "DESKTOP-83BVJ92:20002" } ], version: 2.0 }
d20001| 2024-05-25T09:31:24.359-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55236 #2 (2 connections now open)
d20001| 2024-05-25T09:31:24.394-0500 I  NETWORK  [conn2] end connection 192.168.56.137:55236 (1 connection now open)
d20000| 2024-05-25T09:31:24.395-0500 I  INDEX    [monitoring-keys-for-HMAC] index build: done building index _id_ on ns admin.system.keys
d20002| 2024-05-25T09:31:24.396-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55237 #2 (2 connections now open)
d20000| 2024-05-25T09:31:24.397-0500 I  REPL     [conn1] replSetReconfig config object with 3 members parses ok
d20002| 2024-05-25T09:31:24.397-0500 I  NETWORK  [conn2] end connection 192.168.56.137:55237 (1 connection now open)
d20000| 2024-05-25T09:31:24.422-0500 I  REPL     [conn1] Scheduling remote command request for reconfig quorum check: RemoteCommand 1 -- target:DESKTOP-83BVJ92:20001 db:admin cmd:{ replSetHeartbeat: "MiReplicaSet", configVersion: 2, hbv: 1, from: "DESKTOP-83BVJ92:20000", fromId: 0, term: 1 }
d20000| 2024-05-25T09:31:24.424-0500 I  COMMAND  [monitoring-keys-for-HMAC] command admin.system.keys command: insert { insert: "system.keys", bypassDocumentValidation: false, ordered: true, documents: [ { _id: 7372944802540683266, purpose: "HMAC", key: BinData(0, 0F8A9765EA4FD65A20B88F3C578D76F27601CC1E), expiresAt: Timestamp(1724423484, 0) } ], writeConcern: { w: "majority", wtimeout: 60000 }, allowImplicitCollectionCreation: true, $db: "admin" } ninserted:1 keysInserted:1 numYields:0 reslen:230 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 3 } }, ReplicationStateTransition: { acquireCount: { w: 3 } }, Global: { acquireCount: { w: 3 } }, Database: { acquireCount: { W: 3 } }, Collection: { acquireCount: { r: 2, w: 2, W: 1 } }, Mutex: { acquireCount: { r: 5 } } } flowControl:{ acquireCount: 3, timeAcquiringMicros: 3 } storage:{} protocol:op_msg 206ms
d20000| 2024-05-25T09:31:24.424-0500 I  REPL     [conn1] Scheduling remote command request for reconfig quorum check: RemoteCommand 2 -- target:DESKTOP-83BVJ92:20002 db:admin cmd:{ replSetHeartbeat: "MiReplicaSet", configVersion: 2, hbv: 1, from: "DESKTOP-83BVJ92:20000", fromId: 0, term: 1 }
d20000| 2024-05-25T09:31:24.424-0500 I  CONNPOOL [Replication] Connecting to DESKTOP-83BVJ92:20001
d20000| 2024-05-25T09:31:24.445-0500 I  CONNPOOL [Replication] Connecting to DESKTOP-83BVJ92:20002
d20001| 2024-05-25T09:31:24.446-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55239 #3 (2 connections now open)
d20002| 2024-05-25T09:31:24.509-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55240 #3 (2 connections now open)
d20001| 2024-05-25T09:31:24.509-0500 I  NETWORK  [conn3] received client metadata from 192.168.56.137:55239 conn3: { driver: { name: "NetworkInterfaceTL", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
d20002| 2024-05-25T09:31:24.511-0500 I  NETWORK  [conn3] received client metadata from 192.168.56.137:55240 conn3: { driver: { name: "NetworkInterfaceTL", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
d20002| 2024-05-25T09:31:24.532-0500 I  CONNPOOL [Replication] Connecting to DESKTOP-83BVJ92:20000
d20001| 2024-05-25T09:31:24.532-0500 I  CONNPOOL [Replication] Connecting to DESKTOP-83BVJ92:20000
d20000| 2024-05-25T09:31:24.537-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55241 #6 (2 connections now open)
d20000| 2024-05-25T09:31:24.538-0500 I  NETWORK  [conn6] received client metadata from 192.168.56.137:55241 conn6: { driver: { name: "NetworkInterfaceTL", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
d20000| 2024-05-25T09:31:24.538-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55242 #7 (3 connections now open)
d20000| 2024-05-25T09:31:24.542-0500 I  NETWORK  [conn7] received client metadata from 192.168.56.137:55242 conn7: { driver: { name: "NetworkInterfaceTL", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
d20000| 2024-05-25T09:31:24.543-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55243 #8 (4 connections now open)
d20000| 2024-05-25T09:31:24.545-0500 I  NETWORK  [conn8] end connection 192.168.56.137:55243 (3 connections now open)
d20000| 2024-05-25T09:31:24.545-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55244 #9 (4 connections now open)
d20000| 2024-05-25T09:31:24.546-0500 I  NETWORK  [conn9] end connection 192.168.56.137:55244 (3 connections now open)
d20000| 2024-05-25T09:31:24.581-0500 I  REPL     [conn1] New replica set config in use: { _id: "MiReplicaSet", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "DESKTOP-83BVJ92:20000", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "DESKTOP-83BVJ92:20001", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: "DESKTOP-83BVJ92:20002", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('6651f63be8c43419705d492c') } }
d20000| 2024-05-25T09:31:24.581-0500 I  REPL     [conn1] This node is DESKTOP-83BVJ92:20000 in the config
d20000| 2024-05-25T09:31:24.581-0500 I  COMMAND  [conn1] command local.system.replset appName: "MongoDB Shell" command: replSetReconfig { replSetReconfig: { _id: "MiReplicaSet", protocolVersion: 1.0, members: [ { _id: 0.0, host: "DESKTOP-83BVJ92:20000" }, { _id: 1.0, host: "DESKTOP-83BVJ92:20001" }, { _id: 2.0, host: "DESKTOP-83BVJ92:20002" } ], version: 2.0 }, lsid: { id: UUID("c0238a8b-ad5d-42ed-b9ef-241f8e19144c") }, $clusterTime: { clusterTime: Timestamp(1716647484, 3), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $readPreference: { mode: "secondaryPreferred" }, $db: "admin" } numYields:0 reslen:163 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 2 } }, ReplicationStateTransition: { acquireCount: { w: 2 } }, Global: { acquireCount: { w: 2 } }, Database: { acquireCount: { W: 1 } }, Mutex: { acquireCount: { r: 1 } } } flowControl:{ acquireCount: 2, timeAcquiringMicros: 3 } storage:{} protocol:op_msg 278ms
d20000| 2024-05-25T09:31:24.617-0500 I  CONNPOOL [Replication] Ending connection to host DESKTOP-83BVJ92:20002 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
d20002| 2024-05-25T09:31:24.618-0500 I  NETWORK  [conn3] end connection 192.168.56.137:55240 (1 connection now open)
d20000| 2024-05-25T09:31:24.618-0500 I  REPL     [replexec-0] Member DESKTOP-83BVJ92:20001 is now in state STARTUP
d20002| 2024-05-25T09:31:24.619-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55245 #6 (2 connections now open)
d20002| 2024-05-25T09:31:24.623-0500 I  NETWORK  [conn6] received client metadata from 192.168.56.137:55245 conn6: { driver: { name: "NetworkInterfaceTL", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
d20000| 2024-05-25T09:31:24.623-0500 I  REPL     [replexec-1] Member DESKTOP-83BVJ92:20002 is now in state STARTUP
d20000| 2024-05-25T09:31:25.040-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55246 #11 (4 connections now open)
d20000| 2024-05-25T09:31:25.045-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55247 #12 (5 connections now open)
d20000| 2024-05-25T09:31:25.050-0500 I  NETWORK  [conn11] end connection 192.168.56.137:55246 (4 connections now open)
d20001| 2024-05-25T09:31:25.050-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55248 #6 (3 connections now open)
d20001| 2024-05-25T09:31:25.051-0500 I  NETWORK  [conn6] end connection 192.168.56.137:55248 (2 connections now open)
d20002| 2024-05-25T09:31:25.056-0500 I  STORAGE  [replexec-0] createCollection: local.system.replset with generated UUID: 2cbfc584-ba52-4220-8942-c44202a87acf and options: {}
d20000| 2024-05-25T09:31:25.056-0500 I  NETWORK  [conn12] end connection 192.168.56.137:55247 (3 connections now open)
d20002| 2024-05-25T09:31:25.062-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55249 #9 (3 connections now open)
d20002| 2024-05-25T09:31:25.063-0500 I  NETWORK  [conn9] end connection 192.168.56.137:55249 (2 connections now open)
d20001| 2024-05-25T09:31:25.063-0500 I  STORAGE  [replexec-0] createCollection: local.system.replset with generated UUID: 081614cf-2cde-4b0b-8bc3-69a11832bf8a and options: {}
d20002| 2024-05-25T09:31:25.279-0500 I  INDEX    [replexec-0] index build: done building index _id_ on ns local.system.replset
d20002| 2024-05-25T09:31:25.280-0500 I  REPL     [replexec-0] New replica set config in use: { _id: "MiReplicaSet", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "DESKTOP-83BVJ92:20000", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "DESKTOP-83BVJ92:20001", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: "DESKTOP-83BVJ92:20002", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('6651f63be8c43419705d492c') } }
d20002| 2024-05-25T09:31:25.280-0500 I  REPL     [replexec-0] This node is DESKTOP-83BVJ92:20002 in the config
d20002| 2024-05-25T09:31:25.280-0500 I  REPL     [replexec-0] transition to STARTUP2 from STARTUP
d20002| 2024-05-25T09:31:25.283-0500 I  CONNPOOL [Replication] Connecting to DESKTOP-83BVJ92:20001
d20002| 2024-05-25T09:31:25.283-0500 I  REPL     [replexec-0] Starting replication storage threads
d20002| 2024-05-25T09:31:25.284-0500 I  REPL     [replexec-1] Member DESKTOP-83BVJ92:20000 is now in state PRIMARY
d20001| 2024-05-25T09:31:25.284-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55250 #9 (3 connections now open)
d20001| 2024-05-25T09:31:25.290-0500 I  NETWORK  [conn9] received client metadata from 192.168.56.137:55250 conn9: { driver: { name: "NetworkInterfaceTL", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
d20001| 2024-05-25T09:31:25.291-0500 I  CONNPOOL [Replication] Connecting to DESKTOP-83BVJ92:20002
d20002| 2024-05-25T09:31:25.291-0500 I  REPL     [replexec-3] Member DESKTOP-83BVJ92:20001 is now in state STARTUP
d20002| 2024-05-25T09:31:25.291-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55251 #11 (3 connections now open)
d20002| 2024-05-25T09:31:25.292-0500 I  NETWORK  [conn11] received client metadata from 192.168.56.137:55251 conn11: { driver: { name: "NetworkInterfaceTL", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
d20001| 2024-05-25T09:31:25.308-0500 I  INDEX    [replexec-0] index build: done building index _id_ on ns local.system.replset
d20001| 2024-05-25T09:31:25.309-0500 I  REPL     [replexec-0] New replica set config in use: { _id: "MiReplicaSet", version: 2, protocolVersion: 1, writeConcernMajorityJournalDefault: true, members: [ { _id: 0, host: "DESKTOP-83BVJ92:20000", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 1, host: "DESKTOP-83BVJ92:20001", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: "DESKTOP-83BVJ92:20002", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: -1, catchUpTakeoverDelayMillis: 30000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('6651f63be8c43419705d492c') } }
d20001| 2024-05-25T09:31:25.309-0500 I  REPL     [replexec-0] This node is DESKTOP-83BVJ92:20001 in the config
d20001| 2024-05-25T09:31:25.309-0500 I  REPL     [replexec-0] transition to STARTUP2 from STARTUP
d20001| 2024-05-25T09:31:25.310-0500 I  REPL     [replexec-3] Member DESKTOP-83BVJ92:20000 is now in state PRIMARY
d20001| 2024-05-25T09:31:25.310-0500 I  REPL     [replexec-3] Member DESKTOP-83BVJ92:20002 is now in state STARTUP2
d20001| 2024-05-25T09:31:25.311-0500 I  REPL     [replexec-0] Starting replication storage threads
d20002| 2024-05-25T09:31:25.354-0500 I  STORAGE  [replexec-0] createCollection: local.temp_oplog_buffer with generated UUID: 3e89ed71-377d-4ac5-9e1e-d6d839050476 and options: { temp: true }
d20001| 2024-05-25T09:31:25.379-0500 I  STORAGE  [replexec-0] createCollection: local.temp_oplog_buffer with generated UUID: 3ba28d54-ebff-4f72-88f0-ee20b35c9bca and options: { temp: true }
d20002| 2024-05-25T09:31:25.543-0500 I  INDEX    [replexec-0] index build: done building index _id_ on ns local.temp_oplog_buffer
d20002| 2024-05-25T09:31:25.544-0500 I  INITSYNC [replication-0] Starting initial sync (attempt 1 of 10)
d20002| 2024-05-25T09:31:25.544-0500 I  STORAGE  [replication-0] Finishing collection drop for local.temp_oplog_buffer (3e89ed71-377d-4ac5-9e1e-d6d839050476).
d20001| 2024-05-25T09:31:25.574-0500 I  INDEX    [replexec-0] index build: done building index _id_ on ns local.temp_oplog_buffer
d20001| 2024-05-25T09:31:25.574-0500 I  INITSYNC [replication-0] Starting initial sync (attempt 1 of 10)
d20001| 2024-05-25T09:31:25.575-0500 I  STORAGE  [replication-0] Finishing collection drop for local.temp_oplog_buffer (3ba28d54-ebff-4f72-88f0-ee20b35c9bca).
d20002| 2024-05-25T09:31:25.637-0500 I  STORAGE  [replication-0] createCollection: local.temp_oplog_buffer with generated UUID: be006133-94a2-4302-9059-2d58bb8050d2 and options: { temp: true }
d20001| 2024-05-25T09:31:25.662-0500 I  STORAGE  [replication-0] createCollection: local.temp_oplog_buffer with generated UUID: 0ac861a3-21ee-472e-bfc7-66e2f74afc0d and options: { temp: true }
d20002| 2024-05-25T09:31:25.792-0500 I  REPL     [replexec-0] Member DESKTOP-83BVJ92:20001 is now in state STARTUP2
d20002| 2024-05-25T09:31:25.818-0500 I  INDEX    [replication-0] index build: done building index _id_ on ns local.temp_oplog_buffer
d20002| 2024-05-25T09:31:25.837-0500 I  REPL     [replication-0] sync source candidate: DESKTOP-83BVJ92:20000
d20002| 2024-05-25T09:31:25.837-0500 I  INITSYNC [replication-0] Initial syncer oplog truncation finished in: 0ms
d20002| 2024-05-25T09:31:25.837-0500 I  REPL     [replication-0] ******
d20001| 2024-05-25T09:31:25.838-0500 I  INDEX    [replication-0] index build: done building index _id_ on ns local.temp_oplog_buffer
d20002| 2024-05-25T09:31:25.837-0500 I  REPL     [replication-0] creating replication oplog of size: 40MB...
d20001| 2024-05-25T09:31:25.839-0500 I  REPL     [replication-0] sync source candidate: DESKTOP-83BVJ92:20000
d20001| 2024-05-25T09:31:25.840-0500 I  INITSYNC [replication-0] Initial syncer oplog truncation finished in: 0ms
d20001| 2024-05-25T09:31:25.840-0500 I  REPL     [replication-0] ******
d20001| 2024-05-25T09:31:25.840-0500 I  REPL     [replication-0] creating replication oplog of size: 40MB...
d20002| 2024-05-25T09:31:25.837-0500 I  STORAGE  [replication-0] createCollection: local.oplog.rs with generated UUID: f8ed9bdc-dd6f-48a3-bb03-59e304909d26 and options: { capped: true, size: 41943040, autoIndexId: false }
d20001| 2024-05-25T09:31:25.840-0500 I  STORAGE  [replication-0] createCollection: local.oplog.rs with generated UUID: 2b40b5f9-0def-44a6-b4f4-68bfba2e2d4d and options: { capped: true, size: 41943040, autoIndexId: false }
d20002| 2024-05-25T09:31:25.937-0500 I  STORAGE  [replication-0] Starting OplogTruncaterThread local.oplog.rs
d20002| 2024-05-25T09:31:25.938-0500 I  STORAGE  [replication-0] The size storer reports that the oplog contains 0 records totaling to 0 bytes
d20002| 2024-05-25T09:31:25.938-0500 I  STORAGE  [replication-0] Scanning the oplog to determine where to place markers for truncation
d20002| 2024-05-25T09:31:25.938-0500 I  STORAGE  [replication-0] WiredTiger record store oplog processing took 0ms
d20001| 2024-05-25T09:31:25.961-0500 I  STORAGE  [replication-0] Starting OplogTruncaterThread local.oplog.rs
d20001| 2024-05-25T09:31:25.961-0500 I  STORAGE  [replication-0] The size storer reports that the oplog contains 0 records totaling to 0 bytes
d20001| 2024-05-25T09:31:25.962-0500 I  STORAGE  [replication-0] Scanning the oplog to determine where to place markers for truncation
d20001| 2024-05-25T09:31:25.962-0500 I  STORAGE  [replication-0] WiredTiger record store oplog processing took 0ms
d20002| 2024-05-25T09:31:26.334-0500 I  REPL     [replication-0] ******
d20002| 2024-05-25T09:31:26.335-0500 I  REPL     [replication-0] dropReplicatedDatabases - dropping 1 databases
d20002| 2024-05-25T09:31:26.335-0500 I  REPL     [replication-0] dropReplicatedDatabases - dropped 1 databases
d20002| 2024-05-25T09:31:26.336-0500 I  CONNPOOL [RS] Connecting to DESKTOP-83BVJ92:20000
d20000| 2024-05-25T09:31:26.340-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55252 #13 (4 connections now open)
d20000| 2024-05-25T09:31:26.341-0500 I  NETWORK  [conn13] received client metadata from 192.168.56.137:55252 conn13: { driver: { name: "NetworkInterfaceTL", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
d20000| 2024-05-25T09:31:26.372-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55253 #14 (5 connections now open)
d20000| 2024-05-25T09:31:26.378-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55254 #15 (6 connections now open)
d20000| 2024-05-25T09:31:26.378-0500 I  NETWORK  [conn14] received client metadata from 192.168.56.137:55253 conn14: { driver: { name: "NetworkInterfaceTL", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
d20000| 2024-05-25T09:31:26.379-0500 I  NETWORK  [conn15] received client metadata from 192.168.56.137:55254 conn15: { driver: { name: "MongoDB Internal Client", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
d20001| 2024-05-25T09:31:26.395-0500 I  REPL     [replication-0] ******
d20002| 2024-05-25T09:31:26.396-0500 I  INITSYNC [replication-1] CollectionCloner::start called, on ns:admin.system.keys
d20001| 2024-05-25T09:31:26.396-0500 I  REPL     [replication-0] dropReplicatedDatabases - dropping 1 databases
d20001| 2024-05-25T09:31:26.396-0500 I  REPL     [replication-0] dropReplicatedDatabases - dropped 1 databases
d20000| 2024-05-25T09:31:26.399-0500 I  NETWORK  [conn15] end connection 192.168.56.137:55254 (5 connections now open)
d20002| 2024-05-25T09:31:26.401-0500 I  STORAGE  [repl-writer-worker-0] createCollection: admin.system.keys with provided UUID: 192a8fff-1fcb-4b7e-a8d7-ba8a7902c5c4 and options: { uuid: UUID("192a8fff-1fcb-4b7e-a8d7-ba8a7902c5c4") }
d20001| 2024-05-25T09:31:26.402-0500 I  CONNPOOL [RS] Connecting to DESKTOP-83BVJ92:20000
d20000| 2024-05-25T09:31:26.404-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55255 #16 (6 connections now open)
d20000| 2024-05-25T09:31:26.406-0500 I  NETWORK  [conn16] received client metadata from 192.168.56.137:55255 conn16: { driver: { name: "NetworkInterfaceTL", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
d20000| 2024-05-25T09:31:26.409-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55256 #17 (7 connections now open)
d20000| 2024-05-25T09:31:26.410-0500 I  NETWORK  [conn17] received client metadata from 192.168.56.137:55256 conn17: { driver: { name: "NetworkInterfaceTL", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
d20000| 2024-05-25T09:31:26.410-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55257 #18 (8 connections now open)
d20000| 2024-05-25T09:31:26.412-0500 I  NETWORK  [conn18] received client metadata from 192.168.56.137:55257 conn18: { driver: { name: "MongoDB Internal Client", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
d20001| 2024-05-25T09:31:26.413-0500 I  INITSYNC [replication-1] CollectionCloner::start called, on ns:admin.system.keys
d20001| 2024-05-25T09:31:26.414-0500 I  STORAGE  [repl-writer-worker-0] createCollection: admin.system.keys with provided UUID: 192a8fff-1fcb-4b7e-a8d7-ba8a7902c5c4 and options: { uuid: UUID("192a8fff-1fcb-4b7e-a8d7-ba8a7902c5c4") }
d20000| 2024-05-25T09:31:26.415-0500 I  NETWORK  [conn18] end connection 192.168.56.137:55257 (7 connections now open)
d20002| 2024-05-25T09:31:26.571-0500 I  INDEX    [repl-writer-worker-0] index build: starting on admin.system.keys properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "admin.system.keys" } using method: Foreground
d20002| 2024-05-25T09:31:26.572-0500 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 200 megabytes of RAM
d20000| 2024-05-25T09:31:26.573-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55259 #19 (8 connections now open)
d20000| 2024-05-25T09:31:26.573-0500 I  NETWORK  [conn19] received client metadata from 192.168.56.137:55259 conn19: { driver: { name: "MongoDB Internal Client", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
d20001| 2024-05-25T09:31:26.595-0500 I  INDEX    [repl-writer-worker-0] index build: starting on admin.system.keys properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "admin.system.keys" } using method: Foreground
d20001| 2024-05-25T09:31:26.595-0500 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 200 megabytes of RAM
d20000| 2024-05-25T09:31:26.596-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55260 #20 (9 connections now open)
d20002| 2024-05-25T09:31:26.597-0500 I  INITSYNC [replication-1] CollectionCloner ns:admin.system.keys finished cloning with status: OK
d20000| 2024-05-25T09:31:26.597-0500 I  NETWORK  [conn19] end connection 192.168.56.137:55259 (8 connections now open)
d20000| 2024-05-25T09:31:26.598-0500 I  NETWORK  [conn20] received client metadata from 192.168.56.137:55260 conn20: { driver: { name: "MongoDB Internal Client", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
d20001| 2024-05-25T09:31:26.603-0500 I  INITSYNC [replication-1] CollectionCloner ns:admin.system.keys finished cloning with status: OK
d20000| 2024-05-25T09:31:26.603-0500 I  NETWORK  [conn20] end connection 192.168.56.137:55260 (7 connections now open)
d20000| 2024-05-25T09:31:26.619-0500 I  REPL     [replexec-1] Member DESKTOP-83BVJ92:20001 is now in state STARTUP2
d20000| 2024-05-25T09:31:26.624-0500 I  REPL     [replexec-1] Member DESKTOP-83BVJ92:20002 is now in state STARTUP2
d20002| 2024-05-25T09:31:26.627-0500 I  INDEX    [replication-1] index build: inserted 2 keys from external sorter into index in 0 seconds
d20001| 2024-05-25T09:31:26.627-0500 I  INDEX    [replication-1] index build: inserted 2 keys from external sorter into index in 0 seconds
d20001| 2024-05-25T09:31:26.729-0500 I  INDEX    [replication-1] index build: done building index _id_ on ns admin.system.keys
d20001| 2024-05-25T09:31:26.729-0500 I  INITSYNC [replication-1] CollectionCloner::start called, on ns:admin.system.version
d20002| 2024-05-25T09:31:26.729-0500 I  INDEX    [replication-1] index build: done building index _id_ on ns admin.system.keys
d20001| 2024-05-25T09:31:26.731-0500 I  STORAGE  [repl-writer-worker-0] createCollection: admin.system.version with provided UUID: be3bfec4-3482-4616-a773-7a4e178c6a48 and options: { uuid: UUID("be3bfec4-3482-4616-a773-7a4e178c6a48") }
d20002| 2024-05-25T09:31:26.730-0500 I  INITSYNC [replication-1] CollectionCloner::start called, on ns:admin.system.version
d20002| 2024-05-25T09:31:26.731-0500 I  STORAGE  [repl-writer-worker-0] createCollection: admin.system.version with provided UUID: be3bfec4-3482-4616-a773-7a4e178c6a48 and options: { uuid: UUID("be3bfec4-3482-4616-a773-7a4e178c6a48") }
d20001| 2024-05-25T09:31:26.962-0500 I  INDEX    [repl-writer-worker-0] index build: starting on admin.system.version properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "admin.system.version" } using method: Foreground
d20001| 2024-05-25T09:31:26.962-0500 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 200 megabytes of RAM
d20000| 2024-05-25T09:31:26.963-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55262 #21 (8 connections now open)
d20000| 2024-05-25T09:31:26.964-0500 I  NETWORK  [conn21] received client metadata from 192.168.56.137:55262 conn21: { driver: { name: "MongoDB Internal Client", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
d20001| 2024-05-25T09:31:26.968-0500 I  COMMAND  [repl-writer-worker-0] setting featureCompatibilityVersion to 4.2
d20001| 2024-05-25T09:31:26.968-0500 I  NETWORK  [repl-writer-worker-0] Skip closing connection for connection # 9
d20001| 2024-05-25T09:31:26.968-0500 I  NETWORK  [repl-writer-worker-0] Skip closing connection for connection # 3
d20001| 2024-05-25T09:31:26.968-0500 I  NETWORK  [repl-writer-worker-0] Skip closing connection for connection # 1
d20001| 2024-05-25T09:31:26.968-0500 I  INITSYNC [replication-0] CollectionCloner ns:admin.system.version finished cloning with status: OK
d20001| 2024-05-25T09:31:26.970-0500 I  INDEX    [replication-0] index build: inserted 1 keys from external sorter into index in 0 seconds
d20000| 2024-05-25T09:31:26.968-0500 I  NETWORK  [conn21] end connection 192.168.56.137:55262 (7 connections now open)
d20002| 2024-05-25T09:31:26.985-0500 I  INDEX    [repl-writer-worker-0] index build: starting on admin.system.version properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "admin.system.version" } using method: Foreground
d20002| 2024-05-25T09:31:26.985-0500 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 200 megabytes of RAM
d20000| 2024-05-25T09:31:26.986-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55263 #22 (8 connections now open)
d20000| 2024-05-25T09:31:26.990-0500 I  NETWORK  [conn22] received client metadata from 192.168.56.137:55263 conn22: { driver: { name: "MongoDB Internal Client", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
d20002| 2024-05-25T09:31:26.996-0500 I  COMMAND  [repl-writer-worker-0] setting featureCompatibilityVersion to 4.2
d20002| 2024-05-25T09:31:26.996-0500 I  NETWORK  [repl-writer-worker-0] Skip closing connection for connection # 11
d20000| 2024-05-25T09:31:26.997-0500 I  NETWORK  [conn22] end connection 192.168.56.137:55263 (7 connections now open)
d20002| 2024-05-25T09:31:26.996-0500 I  NETWORK  [repl-writer-worker-0] Skip closing connection for connection # 6
d20002| 2024-05-25T09:31:26.997-0500 I  NETWORK  [repl-writer-worker-0] Skip closing connection for connection # 1
d20002| 2024-05-25T09:31:26.997-0500 I  INITSYNC [replication-0] CollectionCloner ns:admin.system.version finished cloning with status: OK
d20001| 2024-05-25T09:31:27.004-0500 I  INDEX    [replication-0] index build: done building index _id_ on ns admin.system.version
d20002| 2024-05-25T09:31:26.999-0500 I  INDEX    [replication-0] index build: inserted 1 keys from external sorter into index in 0 seconds
d20001| 2024-05-25T09:31:27.025-0500 I  INITSYNC [replication-1] CollectionCloner::start called, on ns:config.image_collection
d20001| 2024-05-25T09:31:27.026-0500 I  STORAGE  [repl-writer-worker-0] createCollection: config.image_collection with provided UUID: b444e2ad-3f31-4d9a-81cc-fe05abfc605e and options: { uuid: UUID("b444e2ad-3f31-4d9a-81cc-fe05abfc605e") }
d20002| 2024-05-25T09:31:27.060-0500 I  INDEX    [replication-0] index build: done building index _id_ on ns admin.system.version
d20002| 2024-05-25T09:31:27.077-0500 I  INITSYNC [replication-1] CollectionCloner::start called, on ns:config.image_collection
d20002| 2024-05-25T09:31:27.077-0500 I  STORAGE  [repl-writer-worker-0] createCollection: config.image_collection with provided UUID: b444e2ad-3f31-4d9a-81cc-fe05abfc605e and options: { uuid: UUID("b444e2ad-3f31-4d9a-81cc-fe05abfc605e") }
d20001| 2024-05-25T09:31:27.436-0500 I  INDEX    [repl-writer-worker-0] index build: starting on config.image_collection properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "config.image_collection" } using method: Hybrid
d20001| 2024-05-25T09:31:27.436-0500 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 200 megabytes of RAM
d20000| 2024-05-25T09:31:27.438-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55264 #23 (8 connections now open)
d20000| 2024-05-25T09:31:27.439-0500 I  NETWORK  [conn23] received client metadata from 192.168.56.137:55264 conn23: { driver: { name: "MongoDB Internal Client", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
d20000| 2024-05-25T09:31:27.445-0500 I  NETWORK  [conn23] end connection 192.168.56.137:55264 (7 connections now open)
d20001| 2024-05-25T09:31:27.445-0500 I  INITSYNC [replication-0] CollectionCloner ns:config.image_collection finished cloning with status: OK
d20001| 2024-05-25T09:31:27.447-0500 I  INDEX    [replication-0] index build: inserted 0 keys from external sorter into index in 0 seconds
d20002| 2024-05-25T09:31:27.454-0500 I  INDEX    [repl-writer-worker-0] index build: starting on config.image_collection properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "config.image_collection" } using method: Hybrid
d20002| 2024-05-25T09:31:27.454-0500 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 200 megabytes of RAM
d20000| 2024-05-25T09:31:27.455-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55265 #24 (8 connections now open)
d20000| 2024-05-25T09:31:27.456-0500 I  NETWORK  [conn24] received client metadata from 192.168.56.137:55265 conn24: { driver: { name: "MongoDB Internal Client", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
d20002| 2024-05-25T09:31:27.461-0500 I  INITSYNC [replication-0] CollectionCloner ns:config.image_collection finished cloning with status: OK
d20000| 2024-05-25T09:31:27.461-0500 I  NETWORK  [conn24] end connection 192.168.56.137:55265 (7 connections now open)
d20002| 2024-05-25T09:31:27.464-0500 I  INDEX    [replication-0] index build: inserted 0 keys from external sorter into index in 0 seconds
d20001| 2024-05-25T09:31:27.473-0500 I  INDEX    [replication-0] index build: done building index _id_ on ns config.image_collection
d20002| 2024-05-25T09:31:27.501-0500 I  INDEX    [replication-0] index build: done building index _id_ on ns config.image_collection
d20001| 2024-05-25T09:31:27.571-0500 I  INITSYNC [replication-0] CollectionCloner::start called, on ns:config.transactions
d20001| 2024-05-25T09:31:27.572-0500 I  STORAGE  [repl-writer-worker-0] createCollection: config.transactions with provided UUID: cde5c6f4-bbd6-4735-bcc0-16f6fa676b79 and options: { uuid: UUID("cde5c6f4-bbd6-4735-bcc0-16f6fa676b79") }
d20002| 2024-05-25T09:31:27.593-0500 I  INITSYNC [replication-0] CollectionCloner::start called, on ns:config.transactions
d20002| 2024-05-25T09:31:27.594-0500 I  STORAGE  [repl-writer-worker-0] createCollection: config.transactions with provided UUID: cde5c6f4-bbd6-4735-bcc0-16f6fa676b79 and options: { uuid: UUID("cde5c6f4-bbd6-4735-bcc0-16f6fa676b79") }
d20001| 2024-05-25T09:31:28.036-0500 I  INDEX    [repl-writer-worker-0] index build: starting on config.transactions properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "config.transactions" } using method: Hybrid
d20001| 2024-05-25T09:31:28.036-0500 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 200 megabytes of RAM
d20000| 2024-05-25T09:31:28.037-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55266 #25 (8 connections now open)
d20000| 2024-05-25T09:31:28.038-0500 I  NETWORK  [conn25] received client metadata from 192.168.56.137:55266 conn25: { driver: { name: "MongoDB Internal Client", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
d20000| 2024-05-25T09:31:28.042-0500 I  NETWORK  [conn25] end connection 192.168.56.137:55266 (7 connections now open)
d20001| 2024-05-25T09:31:28.042-0500 I  INITSYNC [replication-1] CollectionCloner ns:config.transactions finished cloning with status: OK
d20001| 2024-05-25T09:31:28.044-0500 I  INDEX    [replication-1] index build: inserted 0 keys from external sorter into index in 0 seconds
d20001| 2024-05-25T09:31:28.115-0500 I  INDEX    [replication-1] index build: done building index _id_ on ns config.transactions
d20002| 2024-05-25T09:31:28.187-0500 I  INDEX    [repl-writer-worker-0] index build: starting on config.transactions properties: { v: 2, key: { _id: 1 }, name: "_id_", ns: "config.transactions" } using method: Hybrid
d20002| 2024-05-25T09:31:28.187-0500 I  INDEX    [repl-writer-worker-0] build may temporarily use up to 200 megabytes of RAM
d20000| 2024-05-25T09:31:28.188-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55267 #26 (8 connections now open)
d20000| 2024-05-25T09:31:28.199-0500 I  NETWORK  [conn26] received client metadata from 192.168.56.137:55267 conn26: { driver: { name: "MongoDB Internal Client", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
d20000| 2024-05-25T09:31:28.202-0500 I  NETWORK  [conn26] end connection 192.168.56.137:55267 (7 connections now open)
d20002| 2024-05-25T09:31:28.202-0500 I  INITSYNC [replication-1] CollectionCloner ns:config.transactions finished cloning with status: OK
d20002| 2024-05-25T09:31:28.204-0500 I  INDEX    [replication-1] index build: inserted 0 keys from external sorter into index in 0 seconds
d20001| 2024-05-25T09:31:28.212-0500 I  INITSYNC [replication-1] Finished cloning data: OK. Beginning oplog replay.
d20002| 2024-05-25T09:31:28.229-0500 I  INDEX    [replication-1] index build: done building index _id_ on ns config.transactions
d20001| 2024-05-25T09:31:28.229-0500 I  INITSYNC [replication-0] No need to apply operations. (currently at { : Timestamp(1716647484, 6) })
d20001| 2024-05-25T09:31:28.230-0500 I  COMMAND  [replication-1] CMD: collMod: { collMod: "system.replset" }
d20001| 2024-05-25T09:31:28.230-0500 I  COMMAND  [replication-1] CMD: collMod: { collMod: "temp_oplog_buffer" }
d20001| 2024-05-25T09:31:28.230-0500 I  COMMAND  [replication-1] CMD: collMod: { collMod: "system.rollback.id" }
d20001| 2024-05-25T09:31:28.230-0500 I  COMMAND  [replication-1] CMD: collMod: { collMod: "oplog.rs" }
d20001| 2024-05-25T09:31:28.230-0500 I  COMMAND  [replication-1] CMD: collMod: { collMod: "replset.minvalid" }
d20001| 2024-05-25T09:31:28.230-0500 I  COMMAND  [replication-1] CMD: collMod: { collMod: "replset.election" }
d20001| 2024-05-25T09:31:28.230-0500 I  COMMAND  [replication-1] CMD: collMod: { collMod: "replset.oplogTruncateAfterPoint" }
d20001| 2024-05-25T09:31:28.230-0500 I  COMMAND  [replication-1] CMD: collMod: { collMod: "startup_log" }
d20001| 2024-05-25T09:31:28.230-0500 I  INITSYNC [replication-0] Finished fetching oplog during initial sync: CallbackCanceled: error in fetcher batch callback: oplog fetcher is shutting down. Last fetched optime: { ts: Timestamp(0, 0), t: -1 }
d20001| 2024-05-25T09:31:28.231-0500 I  CONNPOOL [RS] Ending connection to host DESKTOP-83BVJ92:20000 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
d20001| 2024-05-25T09:31:28.251-0500 I  INITSYNC [replication-0] Initial sync attempt finishing up.
d20001| 2024-05-25T09:31:28.251-0500 I  INITSYNC [replication-0] Initial Sync Attempt Statistics: { failedInitialSyncAttempts: 0, maxFailedInitialSyncAttempts: 10, initialSyncStart: new Date(1716647485574), totalInitialSyncElapsedMillis: 2677, initialSyncAttempts: [], approxTotalDataSize: 229, approxTotalBytesCopied: 229, remainingInitialSyncEstimatedMillis: 0, fetchedMissingDocs: 0, appliedOps: 0, initialSyncOplogStart: Timestamp(1716647484, 6), initialSyncOplogEnd: Timestamp(1716647484, 6), databases: { databasesToClone: 0, databasesCloned: 2, admin: { collections: 2, clonedCollections: 2, start: new Date(1716647486413), end: new Date(1716647487025), elapsedMillis: 612, admin.system.keys: { documentsToCopy: 2, documentsCopied: 2, indexes: 1, fetchedBatches: 1, bytesToCopy: 170, approxBytesCopied: 170, start: new Date(1716647486413), end: new Date(1716647486730), elapsedMillis: 317, receivedBatches: 1 }, admin.system.version: { documentsToCopy: 1, documentsCopied: 1, indexes: 1, fetchedBatches: 1, bytesToCopy: 59, approxBytesCopied: 59, start: new Date(1716647486729), end: new Date(1716647487025), elapsedMillis: 296, receivedBatches: 1 } }, config: { collections: 2, clonedCollections: 2, start: new Date(1716647487025), end: new Date(1716647488213), elapsedMillis: 1188, config.image_collection: { documentsToCopy: 0, documentsCopied: 0, indexes: 1, fetchedBatches: 0, bytesToCopy: 0, start: new Date(1716647487025), end: new Date(1716647487571), elapsedMillis: 546, receivedBatches: 0 }, config.transactions: { documentsToCopy: 0, documentsCopied: 0, indexes: 1, fetchedBatches: 0, bytesToCopy: 0, start: new Date(1716647487571), end: new Date(1716647488213), elapsedMillis: 642, receivedBatches: 0 } } } }
d20001| 2024-05-25T09:31:28.251-0500 I  STORAGE  [replication-0] Finishing collection drop for local.temp_oplog_buffer (0ac861a3-21ee-472e-bfc7-66e2f74afc0d).
d20002| 2024-05-25T09:31:28.303-0500 I  INITSYNC [replication-1] Finished cloning data: OK. Beginning oplog replay.
d20002| 2024-05-25T09:31:28.307-0500 I  INITSYNC [replication-0] No need to apply operations. (currently at { : Timestamp(1716647484, 6) })
d20002| 2024-05-25T09:31:28.309-0500 I  COMMAND  [replication-1] CMD: collMod: { collMod: "system.replset" }
d20002| 2024-05-25T09:31:28.309-0500 I  COMMAND  [replication-1] CMD: collMod: { collMod: "replset.oplogTruncateAfterPoint" }
d20002| 2024-05-25T09:31:28.309-0500 I  COMMAND  [replication-1] CMD: collMod: { collMod: "replset.election" }
d20002| 2024-05-25T09:31:28.309-0500 I  COMMAND  [replication-1] CMD: collMod: { collMod: "startup_log" }
d20002| 2024-05-25T09:31:28.309-0500 I  COMMAND  [replication-1] CMD: collMod: { collMod: "replset.minvalid" }
d20002| 2024-05-25T09:31:28.309-0500 I  COMMAND  [replication-1] CMD: collMod: { collMod: "temp_oplog_buffer" }
d20002| 2024-05-25T09:31:28.309-0500 I  COMMAND  [replication-1] CMD: collMod: { collMod: "system.rollback.id" }
d20002| 2024-05-25T09:31:28.309-0500 I  COMMAND  [replication-1] CMD: collMod: { collMod: "oplog.rs" }
d20002| 2024-05-25T09:31:28.310-0500 I  INITSYNC [replication-0] Finished fetching oplog during initial sync: CallbackCanceled: error in fetcher batch callback: oplog fetcher is shutting down. Last fetched optime: { ts: Timestamp(0, 0), t: -1 }
d20002| 2024-05-25T09:31:28.310-0500 I  INITSYNC [replication-0] Initial sync attempt finishing up.
d20002| 2024-05-25T09:31:28.310-0500 I  INITSYNC [replication-0] Initial Sync Attempt Statistics: { failedInitialSyncAttempts: 0, maxFailedInitialSyncAttempts: 10, initialSyncStart: new Date(1716647485543), totalInitialSyncElapsedMillis: 2767, initialSyncAttempts: [], approxTotalDataSize: 229, approxTotalBytesCopied: 229, remainingInitialSyncEstimatedMillis: 0, fetchedMissingDocs: 0, appliedOps: 0, initialSyncOplogStart: Timestamp(1716647484, 6), initialSyncOplogEnd: Timestamp(1716647484, 6), databases: { databasesToClone: 0, databasesCloned: 2, admin: { collections: 2, clonedCollections: 2, start: new Date(1716647486394), end: new Date(1716647487076), elapsedMillis: 682, admin.system.keys: { documentsToCopy: 2, documentsCopied: 2, indexes: 1, fetchedBatches: 1, bytesToCopy: 170, approxBytesCopied: 170, start: new Date(1716647486396), end: new Date(1716647486730), elapsedMillis: 334, receivedBatches: 1 }, admin.system.version: { documentsToCopy: 1, documentsCopied: 1, indexes: 1, fetchedBatches: 1, bytesToCopy: 59, approxBytesCopied: 59, start: new Date(1716647486730), end: new Date(1716647487076), elapsedMillis: 346, receivedBatches: 1 } }, config: { collections: 2, clonedCollections: 2, start: new Date(1716647487076), end: new Date(1716647488304), elapsedMillis: 1228, config.image_collection: { documentsToCopy: 0, documentsCopied: 0, indexes: 1, fetchedBatches: 0, bytesToCopy: 0, start: new Date(1716647487077), end: new Date(1716647487593), elapsedMillis: 516, receivedBatches: 0 }, config.transactions: { documentsToCopy: 0, documentsCopied: 0, indexes: 1, fetchedBatches: 0, bytesToCopy: 0, start: new Date(1716647487593), end: new Date(1716647488304), elapsedMillis: 711, receivedBatches: 0 } } } }
d20002| 2024-05-25T09:31:28.310-0500 I  CONNPOOL [RS] Ending connection to host DESKTOP-83BVJ92:20000 due to bad connection status: CallbackCanceled: Callback was canceled; 1 connections to that host remain open
d20002| 2024-05-25T09:31:28.310-0500 I  STORAGE  [replication-0] Finishing collection drop for local.temp_oplog_buffer (be006133-94a2-4302-9059-2d58bb8050d2).
d20001| 2024-05-25T09:31:28.404-0500 I  INITSYNC [replication-0] initial sync done; took 2s.
d20001| 2024-05-25T09:31:28.404-0500 I  REPL     [replication-0] transition to RECOVERING from STARTUP2
d20001| 2024-05-25T09:31:28.404-0500 I  REPL     [replication-0] Starting replication fetcher thread
d20001| 2024-05-25T09:31:28.405-0500 I  REPL     [replication-0] Starting replication applier thread
d20001| 2024-05-25T09:31:28.405-0500 I  REPL     [replication-0] Starting replication reporter thread
d20001| 2024-05-25T09:31:28.405-0500 I  REPL     [rsSync-0] Starting oplog application
d20001| 2024-05-25T09:31:28.406-0500 I  REPL     [rsBackgroundSync] could not find member to sync from
d20001| 2024-05-25T09:31:28.408-0500 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
d20001| 2024-05-25T09:31:28.408-0500 I  REPL     [rsSync-0] Resetting sync source to empty, which was :27017
d20002| 2024-05-25T09:31:28.426-0500 I  INITSYNC [replication-0] initial sync done; took 2s.
d20002| 2024-05-25T09:31:28.426-0500 I  REPL     [replication-0] transition to RECOVERING from STARTUP2
d20002| 2024-05-25T09:31:28.426-0500 I  REPL     [replication-0] Starting replication fetcher thread
d20002| 2024-05-25T09:31:28.427-0500 I  REPL     [replication-0] Starting replication applier thread
d20002| 2024-05-25T09:31:28.427-0500 I  REPL     [replication-0] Starting replication reporter thread
d20002| 2024-05-25T09:31:28.427-0500 I  REPL     [rsSync-0] Starting oplog application
d20002| 2024-05-25T09:31:28.428-0500 I  REPL     [rsBackgroundSync] could not find member to sync from
d20002| 2024-05-25T09:31:28.429-0500 I  REPL     [replexec-3] Member DESKTOP-83BVJ92:20001 is now in state SECONDARY
d20002| 2024-05-25T09:31:28.433-0500 I  REPL     [rsSync-0] transition to SECONDARY from RECOVERING
d20002| 2024-05-25T09:31:28.433-0500 I  REPL     [rsSync-0] Resetting sync source to empty, which was :27017
AwaitNodesAgreeOnPrimary: Waiting for nodes to agree on any primary.
AwaitNodesAgreeOnPrimary: Nodes agreed on primary DESKTOP-83BVJ92:20000
Waiting for keys to sign $clusterTime to be generated
AwaitLastStableRecoveryTimestamp: Beginning for [
        "DESKTOP-83BVJ92:20000",
        "DESKTOP-83BVJ92:20001",
        "DESKTOP-83BVJ92:20002"
]
AwaitNodesAgreeOnPrimary: Waiting for nodes to agree on any primary.
AwaitNodesAgreeOnPrimary: Nodes agreed on primary DESKTOP-83BVJ92:20000
AwaitLastStableRecoveryTimestamp: ensuring the commit point advances for [
        "DESKTOP-83BVJ92:20000",
        "DESKTOP-83BVJ92:20001",
        "DESKTOP-83BVJ92:20002"
]
d20000| 2024-05-25T09:31:28.530-0500 I  NETWORK  [conn16] end connection 192.168.56.137:55255 (6 connections now open)
d20000| 2024-05-25T09:31:28.530-0500 I  NETWORK  [conn13] end connection 192.168.56.137:55252 (5 connections now open)
d20000| 2024-05-25T09:31:28.619-0500 I  REPL     [replexec-1] Member DESKTOP-83BVJ92:20001 is now in state SECONDARY
d20000| 2024-05-25T09:31:28.624-0500 I  REPL     [replexec-1] Member DESKTOP-83BVJ92:20002 is now in state SECONDARY
d20001| 2024-05-25T09:31:28.907-0500 I  STORAGE  [replexec-3] Triggering the first stable checkpoint. Initial Data: Timestamp(1716647484, 6) PrevStable: Timestamp(0, 0) CurrStable: Timestamp(1716647484, 6)
d20001| 2024-05-25T09:31:28.907-0500 I  REPL     [replexec-1] Member DESKTOP-83BVJ92:20002 is now in state SECONDARY
d20002| 2024-05-25T09:31:28.930-0500 I  STORAGE  [replexec-3] Triggering the first stable checkpoint. Initial Data: Timestamp(1716647484, 6) PrevStable: Timestamp(0, 0) CurrStable: Timestamp(1716647484, 6)
d20001| 2024-05-25T09:31:29.407-0500 I  REPL     [rsBackgroundSync] sync source candidate: DESKTOP-83BVJ92:20000
d20001| 2024-05-25T09:31:29.413-0500 I  REPL     [rsBackgroundSync] Changed sync source from empty to DESKTOP-83BVJ92:20000
d20001| 2024-05-25T09:31:29.413-0500 I  REPL     [rsBackgroundSync] scheduling fetcher to read remote oplog on DESKTOP-83BVJ92:20000 starting at filter: { ts: { $gte: Timestamp(1716647484, 6) } }
d20001| 2024-05-25T09:31:29.414-0500 I  CONNPOOL [RS] Connecting to DESKTOP-83BVJ92:20000
d20000| 2024-05-25T09:31:29.417-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55270 #27 (6 connections now open)
d20000| 2024-05-25T09:31:29.418-0500 I  NETWORK  [conn27] received client metadata from 192.168.56.137:55270 conn27: { driver: { name: "NetworkInterfaceTL", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
d20002| 2024-05-25T09:31:29.429-0500 I  REPL     [rsBackgroundSync] sync source candidate: DESKTOP-83BVJ92:20000
d20002| 2024-05-25T09:31:29.430-0500 I  REPL     [rsBackgroundSync] Changed sync source from empty to DESKTOP-83BVJ92:20000
d20002| 2024-05-25T09:31:29.430-0500 I  REPL     [rsBackgroundSync] scheduling fetcher to read remote oplog on DESKTOP-83BVJ92:20000 starting at filter: { ts: { $gte: Timestamp(1716647484, 6) } }
d20002| 2024-05-25T09:31:29.431-0500 I  CONNPOOL [RS] Connecting to DESKTOP-83BVJ92:20000
d20000| 2024-05-25T09:31:29.432-0500 I  NETWORK  [listener] connection accepted from 192.168.56.137:55271 #28 (7 connections now open)
d20000| 2024-05-25T09:31:29.434-0500 I  NETWORK  [conn28] received client metadata from 192.168.56.137:55271 conn28: { driver: { name: "NetworkInterfaceTL", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
d20000| 2024-05-25T09:31:29.496-0500 I  COMMAND  [conn1] command admin.$cmd appName: "MongoDB Shell" command: appendOplogNote { appendOplogNote: 1.0, data: { awaitLastStableRecoveryTimestamp: 1.0 }, writeConcern: { w: "majority", wtimeout: 600000.0 }, lsid: { id: UUID("c0238a8b-ad5d-42ed-b9ef-241f8e19144c") }, $clusterTime: { clusterTime: Timestamp(1716647484, 6), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $readPreference: { mode: "secondaryPreferred" }, $db: "admin" } numYields:0 reslen:163 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 1 } }, ReplicationStateTransition: { acquireCount: { w: 1 } }, Global: { acquireCount: { w: 1 } } } flowControl:{ acquireCount: 1, timeAcquiringMicros: 2 } storage:{} protocol:op_msg 984ms
AwaitLastStableRecoveryTimestamp: Waiting for stable recovery timestamps for [
        "DESKTOP-83BVJ92:20000",
        "DESKTOP-83BVJ92:20001",
        "DESKTOP-83BVJ92:20002"
]
AwaitLastStableRecoveryTimestamp: A stable recovery timestamp has successfully established on [
        "DESKTOP-83BVJ92:20000",
        "DESKTOP-83BVJ92:20001",
        "DESKTOP-83BVJ92:20002"
]


[jsTest] ----
[jsTest] ReplSetTest stepUp: Stepping up DESKTOP-83BVJ92:20000
[jsTest] ----


ReplSetTest awaitReplication: starting: optime for primary, DESKTOP-83BVJ92:20000, is { "ts" : Timestamp(1716647489, 1), "t" : NumberLong(1) }
ReplSetTest awaitReplication: checking secondaries against latest primary optime { "ts" : Timestamp(1716647489, 1), "t" : NumberLong(1) }
ReplSetTest awaitReplication: checking secondary #0: DESKTOP-83BVJ92:20001
ReplSetTest awaitReplication: optime for secondary #0, DESKTOP-83BVJ92:20001, is { "ts" : Timestamp(1716647488, 1), "t" : NumberLong(1) } but latest is { "ts" : Timestamp(1716647489, 1), "t" : NumberLong(1) }
ReplSetTest awaitReplication: secondary #0, DESKTOP-83BVJ92:20001, is NOT synced
ReplSetTest awaitReplication: checking secondaries against latest primary optime { "ts" : Timestamp(1716647489, 1), "t" : NumberLong(1) }
ReplSetTest awaitReplication: checking secondary #0: DESKTOP-83BVJ92:20001
ReplSetTest awaitReplication: secondary #0, DESKTOP-83BVJ92:20001, is synced
ReplSetTest awaitReplication: checking secondary #1: DESKTOP-83BVJ92:20002
ReplSetTest awaitReplication: secondary #1, DESKTOP-83BVJ92:20002, is synced
ReplSetTest awaitReplication: finished: all 2 secondaries synced at optime { "ts" : Timestamp(1716647489, 1), "t" : NumberLong(1) }
d20000| 2024-05-25T09:31:29.733-0500 I  COMMAND  [conn1] Received replSetStepUp request
d20000| 2024-05-25T09:31:29.733-0500 I  ELECTION [conn1] Not starting an election for a replSetStepUp request, since we are not electable due to: Not standing for election again; already primary
AwaitNodesAgreeOnPrimary: Waiting for nodes to agree on any primary.
AwaitNodesAgreeOnPrimary: Nodes agreed on primary DESKTOP-83BVJ92:20000


[jsTest] ----
[jsTest] ReplSetTest stepUp: Finished stepping up DESKTOP-83BVJ92:20000
[jsTest] ----


> conn = new Mongo("localhost:20000")
d20000| 2024-05-25T09:34:31.919-0500 I  NETWORK  [listener] connection accepted from 127.0.0.1:59626 #29 (8 connections now open)
d20000| 2024-05-25T09:34:31.930-0500 I  NETWORK  [conn29] received client metadata from 127.0.0.1:59626 conn29: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
connection to localhost:20000
> testDB = conn.getDB("torneo_futbol_de_salon")
torneo_futbol_de_salon
> testDB.isMaster()
{
        "hosts" : [
                "DESKTOP-83BVJ92:20000",
                "DESKTOP-83BVJ92:20001",
                "DESKTOP-83BVJ92:20002"
        ],
        "setName" : "MiReplicaSet",
        "setVersion" : 2,
        "ismaster" : true,
        "secondary" : false,
        "primary" : "DESKTOP-83BVJ92:20000",
        "me" : "DESKTOP-83BVJ92:20000",
        "electionId" : ObjectId("7fffffff0000000000000001"),
        "lastWrite" : {
                "opTime" : {
                        "ts" : Timestamp(1716647489, 1),
                        "t" : NumberLong(1)
                },
                "lastWriteDate" : ISODate("2024-05-25T14:31:29Z"),
                "majorityOpTime" : {
                        "ts" : Timestamp(1716647489, 1),
                        "t" : NumberLong(1)
                },
                "majorityWriteDate" : ISODate("2024-05-25T14:31:29Z")
        },
        "maxBsonObjectSize" : 16777216,
        "maxMessageSizeBytes" : 48000000,
        "maxWriteBatchSize" : 100000,
        "localTime" : ISODate("2024-05-25T14:36:23.491Z"),
        "logicalSessionTimeoutMinutes" : 30,
        "connectionId" : 29,
        "minWireVersion" : 0,
        "maxWireVersion" : 8,
        "readOnly" : false,
        "ok" : 1,
        "$clusterTime" : {
                "clusterTime" : Timestamp(1716647489, 1),
                "signature" : {
                        "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),
                        "keyId" : NumberLong(0)
                }
        },
        "operationTime" : Timestamp(1716647489, 1)
}
> testDB.arbitros.insert([
...   {
...     "nombre": "Björn Kuipers",
...     "pais": "Países Bajos"
...   },
...   {
...     "nombre": "Cüneyt Çakır",
...     "pais": "Turquía"
...   },
...   {
...     "nombre": "Damir Skomina",
...     "pais": "Eslovenia"
...   }
... ]);
d20000| 2024-05-25T09:39:22.668-0500 I  STORAGE  [conn29] createCollection: torneo_futbol_de_salon.arbitros with generated UUID: a08c0a1f-ba5e-4412-b443-c75523a36441 and options: {}
d20000| 2024-05-25T09:39:23.238-0500 I  INDEX    [conn29] index build: done building index _id_ on ns torneo_futbol_de_salon.arbitros
d20000| 2024-05-25T09:39:23.239-0500 I  COMMAND  [conn29] command torneo_futbol_de_salon.arbitros appName: "MongoDB Shell" command: insert { insert: "arbitros", ordered: true, lsid: { id: UUID("d0b41ff2-43b4-4c18-b097-a03e606accec") }, $clusterTime: { clusterTime: Timestamp(1716647489, 1), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $db: "torneo_futbol_de_salon" } ninserted:3 keysInserted:3 numYields:0 reslen:230 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 4 } }, ReplicationStateTransition: { acquireCount: { w: 4 } }, Global: { acquireCount: { w: 4 } }, Database: { acquireCount: { w: 3, W: 1 } }, Collection: { acquireCount: { r: 2, w: 2, W: 1 } }, Mutex: { acquireCount: { r: 5 } } } flowControl:{ acquireCount: 4, timeAcquiringMicros: 3 } storage:{} protocol:op_msg 571ms
BulkWriteResult({
        "writeErrors" : [ ],
        "writeConcernErrors" : [ ],
        "nInserted" : 3,
        "nUpserted" : 0,
        "nMatched" : 0,
        "nModified" : 0,
        "nRemoved" : 0,
        "upserted" : [ ]
})
> d20002| 2024-05-25T09:39:23.416-0500 I  STORAGE  [repl-writer-worker-1] createCollection: torneo_futbol_de_salon.arbitros with provided UUID: a08c0a1f-ba5e-4412-b443-c75523a36441 and options: { uuid: UUID("a08c0a1f-ba5e-4412-b443-c75523a36441") }
d20001| 2024-05-25T09:39:23.416-0500 I  STORAGE  [repl-writer-worker-1] createCollection: torneo_futbol_de_salon.arbitros with provided UUID: a08c0a1f-ba5e-4412-b443-c75523a36441 and options: { uuid: UUID("a08c0a1f-ba5e-4412-b443-c75523a36441") }
d20001| 2024-05-25T09:39:24.173-0500 I  INDEX    [repl-writer-worker-1] index build: done building index _id_ on ns torneo_futbol_de_salon.arbitros
d20001| 2024-05-25T09:39:24.174-0500 I  REPL     [repl-writer-worker-1] applied op: command { ts: Timestamp(1716647962, 1), t: 1, h: 0, v: 2, op: "c", ns: "torneo_futbol_de_salon.$cmd", ui: UUID("a08c0a1f-ba5e-4412-b443-c75523a36441"), wall: new Date(1716647963238), o: { create: "arbitros", idIndex: { v: 2, key: { _id: 1 }, name: "_id_", ns: "torneo_futbol_de_salon.arbitros" } } }, took 865ms
d20002| 2024-05-25T09:39:24.204-0500 I  INDEX    [repl-writer-worker-1] index build: done building index _id_ on ns torneo_futbol_de_salon.arbitros
d20002| 2024-05-25T09:39:24.204-0500 I  REPL     [repl-writer-worker-1] applied op: command { ts: Timestamp(1716647962, 1), t: 1, h: 0, v: 2, op: "c", ns: "torneo_futbol_de_salon.$cmd", ui: UUID("a08c0a1f-ba5e-4412-b443-c75523a36441"), wall: new Date(1716647963238), o: { create: "arbitros", idIndex: { v: 2, key: { _id: 1 }, name: "_id_", ns: "torneo_futbol_de_salon.arbitros" } } }, took 896ms

> d20000| 2024-05-25T09:41:30.843-0500 I  QUERY    [clientcursormon] Cursor id 5304337292696066081 timed out, idle since 2024-05-25T09:31:28.530-0500
d20000| 2024-05-25T09:41:30.843-0500 I  QUERY    [clientcursormon] Cursor id 13448035583233655 timed out, idle since 2024-05-25T09:31:28.530-0500

> testDB.adeportistas.insert([
...   {
...     "nombre": "Mohamed Salah",
...     "edad": 29,
...     "posicion": "Delantero",
...     "equipo": "Liverpool"
...   },
...   {
...     "nombre": "Kevin De Bruyne",
...     "edad": 30,
...     "posicion": "Centrocampista",
...     "equipo": "Manchester City"
...   },
...   {
...     "nombre": "Sadio Mane",
...     "edad": 29,
...     "posicion": "Delantero",
...     "equipo": "Liverpool"
...   }
... ]);
d20000| 2024-05-25T09:44:17.024-0500 I  STORAGE  [conn29] createCollection: torneo_futbol_de_salon.adeportistas with generated UUID: f504a087-b5bb-4f3a-a4ab-a6ebe5be0f98 and options: {}
d20000| 2024-05-25T09:44:17.118-0500 I  INDEX    [conn29] index build: done building index _id_ on ns torneo_futbol_de_salon.adeportistas
BulkWriteResult({
        "writeErrors" : [ ],
        "writeConcernErrors" : [ ],
        "nInserted" : 3,
        "nUpserted" : 0,
        "nMatched" : 0,
        "nModified" : 0,
        "nRemoved" : 0,
        "upserted" : [ ]
})
> d20002| 2024-05-25T09:44:17.142-0500 I  STORAGE  [repl-writer-worker-4] createCollection: torneo_futbol_de_salon.adeportistas with provided UUID: f504a087-b5bb-4f3a-a4ab-a6ebe5be0f98 and options: { uuid: UUID("f504a087-b5bb-4f3a-a4ab-a6ebe5be0f98") }
d20001| 2024-05-25T09:44:17.142-0500 I  STORAGE  [repl-writer-worker-4] createCollection: torneo_futbol_de_salon.adeportistas with provided UUID: f504a087-b5bb-4f3a-a4ab-a6ebe5be0f98 and options: { uuid: UUID("f504a087-b5bb-4f3a-a4ab-a6ebe5be0f98") }
d20001| 2024-05-25T09:44:17.393-0500 I  INDEX    [repl-writer-worker-4] index build: done building index _id_ on ns torneo_futbol_de_salon.adeportistas
d20001| 2024-05-25T09:44:17.393-0500 I  REPL     [repl-writer-worker-4] applied op: command { ts: Timestamp(1716648257, 1), t: 1, h: 0, v: 2, op: "c", ns: "torneo_futbol_de_salon.$cmd", ui: UUID("f504a087-b5bb-4f3a-a4ab-a6ebe5be0f98"), wall: new Date(1716648257119), o: { create: "adeportistas", idIndex: { v: 2, key: { _id: 1 }, name: "_id_", ns: "torneo_futbol_de_salon.adeportistas" } } }, took 251ms
d20002| 2024-05-25T09:44:17.411-0500 I  INDEX    [repl-writer-worker-4] index build: done building index _id_ on ns torneo_futbol_de_salon.adeportistas
d20002| 2024-05-25T09:44:17.411-0500 I  REPL     [repl-writer-worker-4] applied op: command { ts: Timestamp(1716648257, 1), t: 1, h: 0, v: 2, op: "c", ns: "torneo_futbol_de_salon.$cmd", ui: UUID("f504a087-b5bb-4f3a-a4ab-a6ebe5be0f98"), wall: new Date(1716648257119), o: { create: "adeportistas", idIndex: { v: 2, key: { _id: 1 }, name: "_id_", ns: "torneo_futbol_de_salon.adeportistas" } } }, took 269ms

> testDB.encuentros.insert([
...   {
...     "equipo_local": "Atletico Madrid",
...     "equipo_visitante": "Barcelona",
...     "fecha": "2024-05-06",
...     "resultado": "1-1",
...     "arbitro": "Björn Kuipers"
...   },
...   {
...     "equipo_local": "Manchester City",
...     "equipo_visitante": "Liverpool",
...     "fecha": "2024-05-07",
...     "resultado": "2-3",
...     "arbitro": "Cüneyt Çakır"
...   },
...   {
...     "equipo_local": "Juventus",
...     "equipo_visitante": "Bayern Munich",
...     "fecha": "2024-05-08",
...     "resultado": "0-2",
...     "arbitro": "Damir Skomina"
...   }
... ]);
d20000| 2024-05-25T09:45:17.205-0500 I  STORAGE  [conn29] createCollection: torneo_futbol_de_salon.encuentros with generated UUID: b068e202-4217-4444-9458-bacbf37c7beb and options: {}
d20000| 2024-05-25T09:45:17.297-0500 I  INDEX    [conn29] index build: done building index _id_ on ns torneo_futbol_de_salon.encuentros
BulkWriteResult({
        "writeErrors" : [ ],
        "writeConcernErrors" : [ ],
        "nInserted" : 3,
        "nUpserted" : 0,
        "nMatched" : 0,
        "nModified" : 0,
        "nRemoved" : 0,
        "upserted" : [ ]
})
> d20001| 2024-05-25T09:45:17.318-0500 I  STORAGE  [repl-writer-worker-5] createCollection: torneo_futbol_de_salon.encuentros with provided UUID: b068e202-4217-4444-9458-bacbf37c7beb and options: { uuid: UUID("b068e202-4217-4444-9458-bacbf37c7beb") }
d20002| 2024-05-25T09:45:17.318-0500 I  STORAGE  [repl-writer-worker-5] createCollection: torneo_futbol_de_salon.encuentros with provided UUID: b068e202-4217-4444-9458-bacbf37c7beb and options: { uuid: UUID("b068e202-4217-4444-9458-bacbf37c7beb") }
d20002| 2024-05-25T09:45:17.605-0500 I  INDEX    [repl-writer-worker-5] index build: done building index _id_ on ns torneo_futbol_de_salon.encuentros
d20002| 2024-05-25T09:45:17.606-0500 I  REPL     [repl-writer-worker-5] applied op: command { ts: Timestamp(1716648317, 1), t: 1, h: 0, v: 2, op: "c", ns: "torneo_futbol_de_salon.$cmd", ui: UUID("b068e202-4217-4444-9458-bacbf37c7beb"), wall: new Date(1716648317297), o: { create: "encuentros", idIndex: { v: 2, key: { _id: 1 }, name: "_id_", ns: "torneo_futbol_de_salon.encuentros" } } }, took 289ms
d20001| 2024-05-25T09:45:17.633-0500 I  INDEX    [repl-writer-worker-5] index build: done building index _id_ on ns torneo_futbol_de_salon.encuentros
d20001| 2024-05-25T09:45:17.633-0500 I  REPL     [repl-writer-worker-5] applied op: command { ts: Timestamp(1716648317, 1), t: 1, h: 0, v: 2, op: "c", ns: "torneo_futbol_de_salon.$cmd", ui: UUID("b068e202-4217-4444-9458-bacbf37c7beb"), wall: new Date(1716648317297), o: { create: "encuentros", idIndex: { v: 2, key: { _id: 1 }, name: "_id_", ns: "torneo_futbol_de_salon.encuentros" } } }, took 316ms

> testDB.entrenadores.insert([
...   {
...     "nombre": "Massimiliano Allegri",
...     "equipo": "Juventus"
...   },
...   {
...     "nombre": "Antonio Conte",
...     "equipo": "Inter de Milán"
...   },
...   {
...     "nombre": "Thomas Tuchel",
...     "equipo": "Chelsea"
...   }
... ]);
d20000| 2024-05-25T09:46:05.109-0500 I  STORAGE  [conn29] createCollection: torneo_futbol_de_salon.entrenadores with generated UUID: 1ec907ba-12fa-4247-a41b-59a87ed66f9f and options: {}
d20000| 2024-05-25T09:46:05.195-0500 I  INDEX    [conn29] index build: done building index _id_ on ns torneo_futbol_de_salon.entrenadores
BulkWriteResult({
        "writeErrors" : [ ],
        "writeConcernErrors" : [ ],
        "nInserted" : 3,
        "nUpserted" : 0,
        "nMatched" : 0,
        "nModified" : 0,
        "nRemoved" : 0,
        "upserted" : [ ]
})
> d20001| 2024-05-25T09:46:05.216-0500 I  STORAGE  [repl-writer-worker-7] createCollection: torneo_futbol_de_salon.entrenadores with provided UUID: 1ec907ba-12fa-4247-a41b-59a87ed66f9f and options: { uuid: UUID("1ec907ba-12fa-4247-a41b-59a87ed66f9f") }
d20002| 2024-05-25T09:46:05.216-0500 I  STORAGE  [repl-writer-worker-7] createCollection: torneo_futbol_de_salon.entrenadores with provided UUID: 1ec907ba-12fa-4247-a41b-59a87ed66f9f and options: { uuid: UUID("1ec907ba-12fa-4247-a41b-59a87ed66f9f") }
d20001| 2024-05-25T09:46:05.482-0500 I  INDEX    [repl-writer-worker-7] index build: done building index _id_ on ns torneo_futbol_de_salon.entrenadores
d20001| 2024-05-25T09:46:05.482-0500 I  REPL     [repl-writer-worker-7] applied op: command { ts: Timestamp(1716648365, 1), t: 1, h: 0, v: 2, op: "c", ns: "torneo_futbol_de_salon.$cmd", ui: UUID("1ec907ba-12fa-4247-a41b-59a87ed66f9f"), wall: new Date(1716648365195), o: { create: "entrenadores", idIndex: { v: 2, key: { _id: 1 }, name: "_id_", ns: "torneo_futbol_de_salon.entrenadores" } } }, took 267ms
d20002| 2024-05-25T09:46:05.504-0500 I  INDEX    [repl-writer-worker-7] index build: done building index _id_ on ns torneo_futbol_de_salon.entrenadores
d20002| 2024-05-25T09:46:05.504-0500 I  REPL     [repl-writer-worker-7] applied op: command { ts: Timestamp(1716648365, 1), t: 1, h: 0, v: 2, op: "c", ns: "torneo_futbol_de_salon.$cmd", ui: UUID("1ec907ba-12fa-4247-a41b-59a87ed66f9f"), wall: new Date(1716648365195), o: { create: "entrenadores", idIndex: { v: 2, key: { _id: 1 }, name: "_id_", ns: "torneo_futbol_de_salon.entrenadores" } } }, took 289ms

> testDB.equipos.insert([
...   {
...     "nombre": "Liverpool",
...     "ciudad": "Liverpool",
...     "pais": "Inglaterra",
...     "jugadores": [
...       "Mohamed Salah",
...       "Sadio Mane",
...       "Virgil van Dijk"
...     ]
...   },
...   {
...     "nombre": "Manchester City",
...     "ciudad": "Manchester",
...     "pais": "Inglaterra",
...     "jugadores": [
...       "Kevin De Bruyne",
...       "Raheem Sterling",
...       "Phil Foden"
...     ]
...   },
...   {
...     "nombre": "Juventus",
...     "ciudad": "Turín",
...     "pais": "Italia",
...     "jugadores": [
...       "Cristiano Ronaldo",
...       "Paulo Dybala",
...       "Matthijs de Ligt"
...     ]
...   }
... ]);
d20000| 2024-05-25T09:47:07.191-0500 I  STORAGE  [conn29] createCollection: torneo_futbol_de_salon.equipos with generated UUID: 15abb9c9-88f7-4c5a-8694-4bf250eadfc9 and options: {}
d20000| 2024-05-25T09:47:07.337-0500 I  INDEX    [conn29] index build: done building index _id_ on ns torneo_futbol_de_salon.equipos
d20000| 2024-05-25T09:47:07.338-0500 I  COMMAND  [conn29] command torneo_futbol_de_salon.equipos appName: "MongoDB Shell" command: insert { insert: "equipos", ordered: true, lsid: { id: UUID("d0b41ff2-43b4-4c18-b097-a03e606accec") }, $clusterTime: { clusterTime: Timestamp(1716648365, 4), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $db: "torneo_futbol_de_salon" } ninserted:3 keysInserted:3 numYields:0 reslen:230 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 3 } }, ReplicationStateTransition: { acquireCount: { w: 3 } }, Global: { acquireCount: { w: 3 } }, Database: { acquireCount: { w: 3 } }, Collection: { acquireCount: { r: 2, w: 2, W: 1 } }, Mutex: { acquireCount: { r: 7 } } } flowControl:{ acquireCount: 3, timeAcquiringMicros: 3 } storage:{} protocol:op_msg 147ms
BulkWriteResult({
        "writeErrors" : [ ],
        "writeConcernErrors" : [ ],
        "nInserted" : 3,
        "nUpserted" : 0,
        "nMatched" : 0,
        "nModified" : 0,
        "nRemoved" : 0,
        "upserted" : [ ]
})
> d20002| 2024-05-25T09:47:07.362-0500 I  STORAGE  [repl-writer-worker-8] createCollection: torneo_futbol_de_salon.equipos with provided UUID: 15abb9c9-88f7-4c5a-8694-4bf250eadfc9 and options: { uuid: UUID("15abb9c9-88f7-4c5a-8694-4bf250eadfc9") }
d20001| 2024-05-25T09:47:07.362-0500 I  STORAGE  [repl-writer-worker-8] createCollection: torneo_futbol_de_salon.equipos with provided UUID: 15abb9c9-88f7-4c5a-8694-4bf250eadfc9 and options: { uuid: UUID("15abb9c9-88f7-4c5a-8694-4bf250eadfc9") }
d20001| 2024-05-25T09:47:07.646-0500 I  INDEX    [repl-writer-worker-8] index build: done building index _id_ on ns torneo_futbol_de_salon.equipos
d20001| 2024-05-25T09:47:07.646-0500 I  REPL     [repl-writer-worker-8] applied op: command { ts: Timestamp(1716648427, 1), t: 1, h: 0, v: 2, op: "c", ns: "torneo_futbol_de_salon.$cmd", ui: UUID("15abb9c9-88f7-4c5a-8694-4bf250eadfc9"), wall: new Date(1716648427337), o: { create: "equipos", idIndex: { v: 2, key: { _id: 1 }, name: "_id_", ns: "torneo_futbol_de_salon.equipos" } } }, took 284ms
d20002| 2024-05-25T09:47:07.668-0500 I  INDEX    [repl-writer-worker-8] index build: done building index _id_ on ns torneo_futbol_de_salon.equipos
d20002| 2024-05-25T09:47:07.669-0500 I  REPL     [repl-writer-worker-8] applied op: command { ts: Timestamp(1716648427, 1), t: 1, h: 0, v: 2, op: "c", ns: "torneo_futbol_de_salon.$cmd", ui: UUID("15abb9c9-88f7-4c5a-8694-4bf250eadfc9"), wall: new Date(1716648427337), o: { create: "equipos", idIndex: { v: 2, key: { _id: 1 }, name: "_id_", ns: "torneo_futbol_de_salon.equipos" } } }, took 307ms

> testDB.resultados.insert([
...   {
...     "encuentro_id": "6",
...     "equipo_local": "Atletico Madrid",
...     "goles_local": 1,
...     "equipo_visitante": "Barcelona",
...     "goles_visitante": 1
...   },
...   {
...     "encuentro_id": "7",
...     "equipo_local": "Manchester City",
...     "goles_local": 2,
...     "equipo_visitante": "Liverpool",
...     "goles_visitante": 3
...   },
...   {
...     "encuentro_id": "8",
...     "equipo_local": "Juventus",
...     "goles_local": 0,
...     "equipo_visitante": "Bayern Munich",
...     "goles_visitante": 2
...   }
... ]);
d20000| 2024-05-25T09:47:48.935-0500 I  STORAGE  [conn29] createCollection: torneo_futbol_de_salon.resultados with generated UUID: 0ad171aa-861f-4e64-99ef-28c03b2c7a10 and options: {}
d20000| 2024-05-25T09:47:49.053-0500 I  INDEX    [conn29] index build: done building index _id_ on ns torneo_futbol_de_salon.resultados
d20000| 2024-05-25T09:47:49.053-0500 I  COMMAND  [conn29] command torneo_futbol_de_salon.resultados appName: "MongoDB Shell" command: insert { insert: "resultados", ordered: true, lsid: { id: UUID("d0b41ff2-43b4-4c18-b097-a03e606accec") }, $clusterTime: { clusterTime: Timestamp(1716648427, 4), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $db: "torneo_futbol_de_salon" } ninserted:3 keysInserted:3 numYields:0 reslen:230 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 3 } }, ReplicationStateTransition: { acquireCount: { w: 3 } }, Global: { acquireCount: { w: 3 } }, Database: { acquireCount: { w: 3 } }, Collection: { acquireCount: { r: 2, w: 2, W: 1 } }, Mutex: { acquireCount: { r: 7 } } } flowControl:{ acquireCount: 3, timeAcquiringMicros: 4 } storage:{} protocol:op_msg 118ms
BulkWriteResult({
        "writeErrors" : [ ],
        "writeConcernErrors" : [ ],
        "nInserted" : 3,
        "nUpserted" : 0,
        "nMatched" : 0,
        "nModified" : 0,
        "nRemoved" : 0,
        "upserted" : [ ]
})
> d20002| 2024-05-25T09:47:49.076-0500 I  STORAGE  [repl-writer-worker-9] createCollection: torneo_futbol_de_salon.resultados with provided UUID: 0ad171aa-861f-4e64-99ef-28c03b2c7a10 and options: { uuid: UUID("0ad171aa-861f-4e64-99ef-28c03b2c7a10") }
d20001| 2024-05-25T09:47:49.076-0500 I  STORAGE  [repl-writer-worker-8] createCollection: torneo_futbol_de_salon.resultados with provided UUID: 0ad171aa-861f-4e64-99ef-28c03b2c7a10 and options: { uuid: UUID("0ad171aa-861f-4e64-99ef-28c03b2c7a10") }
d20001| 2024-05-25T09:47:49.285-0500 I  INDEX    [repl-writer-worker-8] index build: done building index _id_ on ns torneo_futbol_de_salon.resultados
d20001| 2024-05-25T09:47:49.286-0500 I  REPL     [repl-writer-worker-8] applied op: command { ts: Timestamp(1716648468, 1), t: 1, h: 0, v: 2, op: "c", ns: "torneo_futbol_de_salon.$cmd", ui: UUID("0ad171aa-861f-4e64-99ef-28c03b2c7a10"), wall: new Date(1716648469053), o: { create: "resultados", idIndex: { v: 2, key: { _id: 1 }, name: "_id_", ns: "torneo_futbol_de_salon.resultados" } } }, took 210ms
d20002| 2024-05-25T09:47:49.367-0500 I  INDEX    [repl-writer-worker-9] index build: done building index _id_ on ns torneo_futbol_de_salon.resultados
d20002| 2024-05-25T09:47:49.367-0500 I  REPL     [repl-writer-worker-9] applied op: command { ts: Timestamp(1716648468, 1), t: 1, h: 0, v: 2, op: "c", ns: "torneo_futbol_de_salon.$cmd", ui: UUID("0ad171aa-861f-4e64-99ef-28c03b2c7a10"), wall: new Date(1716648469053), o: { create: "resultados", idIndex: { v: 2, key: { _id: 1 }, name: "_id_", ns: "torneo_futbol_de_salon.resultados" } } }, took 291ms

> testDB.tabla_posiciones.insert([
...   {
...     "equipo": "Liverpool",
...     "partidos_jugados": 2,
...     "victorias": 2,
...     "empates": 0,
...     "derrotas": 0,
...     "goles_a_favor": 5,
...     "goles_en_contra": 2,
...     "puntos": 6
...   },
...   {
...     "equipo": "Manchester City",
...     "partidos_jugados": 2,
...     "victorias": 0,
...     "empates": 0,
...     "derrotas": 2,
...     "goles_a_favor": 2,
...     "goles_en_contra": 5,
...     "puntos": 0
...   },
...   {
...     "equipo": "Juventus",
...     "partidos_jugados": 1,
...     "victorias": 0,
...     "empates": 0,
...     "derrotas": 1,
...     "goles_a_favor": 0,
...     "goles_en_contra": 2,
...     "puntos": 0
...   }
... ]);
d20000| 2024-05-25T09:48:23.322-0500 I  STORAGE  [conn29] createCollection: torneo_futbol_de_salon.tabla_posiciones with generated UUID: a2299ce3-c31e-4b0e-b5e3-2e2353785850 and options: {}
d20000| 2024-05-25T09:48:23.507-0500 I  INDEX    [conn29] index build: done building index _id_ on ns torneo_futbol_de_salon.tabla_posiciones
d20000| 2024-05-25T09:48:23.507-0500 I  COMMAND  [conn29] command torneo_futbol_de_salon.tabla_posiciones appName: "MongoDB Shell" command: insert { insert: "tabla_posiciones", ordered: true, lsid: { id: UUID("d0b41ff2-43b4-4c18-b097-a03e606accec") }, $clusterTime: { clusterTime: Timestamp(1716648469, 3), signature: { hash: BinData(0, 0000000000000000000000000000000000000000), keyId: 0 } }, $db: "torneo_futbol_de_salon" } ninserted:3 keysInserted:3 numYields:0 reslen:230 locks:{ ParallelBatchWriterMode: { acquireCount: { r: 3 } }, ReplicationStateTransition: { acquireCount: { w: 3 } }, Global: { acquireCount: { w: 3 } }, Database: { acquireCount: { w: 3 } }, Collection: { acquireCount: { r: 2, w: 2, W: 1 } }, Mutex: { acquireCount: { r: 7 } } } flowControl:{ acquireCount: 3, timeAcquiringMicros: 2 } storage:{} protocol:op_msg 185ms
BulkWriteResult({
        "writeErrors" : [ ],
        "writeConcernErrors" : [ ],
        "nInserted" : 3,
        "nUpserted" : 0,
        "nMatched" : 0,
        "nModified" : 0,
        "nRemoved" : 0,
        "upserted" : [ ]
})
> d20001| 2024-05-25T09:48:23.542-0500 I  STORAGE  [repl-writer-worker-11] createCollection: torneo_futbol_de_salon.tabla_posiciones with provided UUID: a2299ce3-c31e-4b0e-b5e3-2e2353785850 and options: { uuid: UUID("a2299ce3-c31e-4b0e-b5e3-2e2353785850") }
d20002| 2024-05-25T09:48:23.542-0500 I  STORAGE  [repl-writer-worker-9] createCollection: torneo_futbol_de_salon.tabla_posiciones with provided UUID: a2299ce3-c31e-4b0e-b5e3-2e2353785850 and options: { uuid: UUID("a2299ce3-c31e-4b0e-b5e3-2e2353785850") }
d20001| 2024-05-25T09:48:23.726-0500 I  INDEX    [repl-writer-worker-11] index build: done building index _id_ on ns torneo_futbol_de_salon.tabla_posiciones
d20001| 2024-05-25T09:48:23.727-0500 I  REPL     [repl-writer-worker-11] applied op: command { ts: Timestamp(1716648503, 1), t: 1, h: 0, v: 2, op: "c", ns: "torneo_futbol_de_salon.$cmd", ui: UUID("a2299ce3-c31e-4b0e-b5e3-2e2353785850"), wall: new Date(1716648503507), o: { create: "tabla_posiciones", idIndex: { v: 2, key: { _id: 1 }, name: "_id_", ns: "torneo_futbol_de_salon.tabla_posiciones" } } }, took 184ms
d20002| 2024-05-25T09:48:23.752-0500 I  INDEX    [repl-writer-worker-9] index build: done building index _id_ on ns torneo_futbol_de_salon.tabla_posiciones
d20002| 2024-05-25T09:48:23.752-0500 I  REPL     [repl-writer-worker-9] applied op: command { ts: Timestamp(1716648503, 1), t: 1, h: 0, v: 2, op: "c", ns: "torneo_futbol_de_salon.$cmd", ui: UUID("a2299ce3-c31e-4b0e-b5e3-2e2353785850"), wall: new Date(1716648503507), o: { create: "tabla_posiciones", idIndex: { v: 2, key: { _id: 1 }, name: "_id_", ns: "torneo_futbol_de_salon.tabla_posiciones" } } }, took 210ms

> testDB.arbitros.count();
3
> testDB.deportistas.count();
0
> testDB.adeportistas.count();
3
> testDB.encuentros.count();
3
> testDB.entrenadores.count();
3
> testDB.equipos.count();
3
> testDB.resultados.count();
3
> testDB.tabla_posiciones.count();
3
>
>
> connSecondary = new Mongo("localhost:20001")
d20001| 2024-05-25T09:56:24.387-0500 I  NETWORK  [listener] connection accepted from 127.0.0.1:60803 #19 (4 connections now open)
d20001| 2024-05-25T09:56:24.388-0500 I  NETWORK  [conn19] received client metadata from 127.0.0.1:60803 conn19: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
connection to localhost:20001
> secondaryTestDB = connSecondary.getDB("torneo_futbol_de_salon")
torneo_futbol_de_salon
> secondaryTestDB.isMaster()
{
        "hosts" : [
                "DESKTOP-83BVJ92:20000",
                "DESKTOP-83BVJ92:20001",
                "DESKTOP-83BVJ92:20002"
        ],
        "setName" : "MiReplicaSet",
        "setVersion" : 2,
        "ismaster" : false,
        "secondary" : true,
        "primary" : "DESKTOP-83BVJ92:20000",
        "me" : "DESKTOP-83BVJ92:20001",
        "lastWrite" : {
                "opTime" : {
                        "ts" : Timestamp(1716648503, 4),
                        "t" : NumberLong(1)
                },
                "lastWriteDate" : ISODate("2024-05-25T14:48:23Z"),
                "majorityOpTime" : {
                        "ts" : Timestamp(1716648503, 4),
                        "t" : NumberLong(1)
                },
                "majorityWriteDate" : ISODate("2024-05-25T14:48:23Z")
        },
        "maxBsonObjectSize" : 16777216,
        "maxMessageSizeBytes" : 48000000,
        "maxWriteBatchSize" : 100000,
        "localTime" : ISODate("2024-05-25T14:58:21.666Z"),
        "logicalSessionTimeoutMinutes" : 30,
        "connectionId" : 19,
        "minWireVersion" : 0,
        "maxWireVersion" : 8,
        "readOnly" : false,
        "ok" : 1,
        "$clusterTime" : {
                "clusterTime" : Timestamp(1716648503, 4),
                "signature" : {
                        "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),
                        "keyId" : NumberLong(0)
                }
        },
        "operationTime" : Timestamp(1716648503, 4)
}
> secondaryTestDB.arbitros.count();
2024-05-25T10:00:08.636-0500 E  QUERY    [js] uncaught exception: Error: count failed: {
        "operationTime" : Timestamp(1716648503, 4),
        "ok" : 0,
        "errmsg" : "not master and slaveOk=false",
        "code" : 13435,
        "codeName" : "NotPrimaryNoSecondaryOk",
        "$clusterTime" : {
                "clusterTime" : Timestamp(1716648503, 4),
                "signature" : {
                        "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),
                        "keyId" : NumberLong(0)
                }
        }
} :
_getErrorWithCode@src/mongo/shell/utils.js:25:13
DBQuery.prototype.count@src/mongo/shell/query.js:376:11
DBCollection.prototype.count@src/mongo/shell/collection.js:1401:12
@(shell):1:1
>
> connPrimary = new Mongo("localhost:20000")
d20000| 2024-05-25T10:01:45.034-0500 I  NETWORK  [listener] connection accepted from 127.0.0.1:61087 #30 (9 connections now open)
d20000| 2024-05-25T10:01:45.034-0500 I  NETWORK  [conn30] received client metadata from 127.0.0.1:61087 conn30: { application: { name: "MongoDB Shell" }, driver: { name: "MongoDB Internal Client", version: "4.2.25" }, os: { type: "Windows", name: "Microsoft Windows 10", architecture: "x86_64", version: "10.0 (build 19045)" } }
connection to localhost:20000
>
> primaryDB = connPrimary.getDB("torneo_futbol_de_salon")
torneo_futbol_de_salon
>
> primaryDB.isMaster()
{
        "hosts" : [
                "DESKTOP-83BVJ92:20000",
                "DESKTOP-83BVJ92:20001",
                "DESKTOP-83BVJ92:20002"
        ],
        "setName" : "MiReplicaSet",
        "setVersion" : 2,
        "ismaster" : true,
        "secondary" : false,
        "primary" : "DESKTOP-83BVJ92:20000",
        "me" : "DESKTOP-83BVJ92:20000",
        "electionId" : ObjectId("7fffffff0000000000000001"),
        "lastWrite" : {
                "opTime" : {
                        "ts" : Timestamp(1716648503, 4),
                        "t" : NumberLong(1)
                },
                "lastWriteDate" : ISODate("2024-05-25T14:48:23Z"),
                "majorityOpTime" : {
                        "ts" : Timestamp(1716648503, 4),
                        "t" : NumberLong(1)
                },
                "majorityWriteDate" : ISODate("2024-05-25T14:48:23Z")
        },
        "maxBsonObjectSize" : 16777216,
        "maxMessageSizeBytes" : 48000000,
        "maxWriteBatchSize" : 100000,
        "localTime" : ISODate("2024-05-25T15:03:32.608Z"),
        "logicalSessionTimeoutMinutes" : 30,
        "connectionId" : 30,
        "minWireVersion" : 0,
        "maxWireVersion" : 8,
        "readOnly" : false,
        "ok" : 1,
        "$clusterTime" : {
                "clusterTime" : Timestamp(1716648503, 4),
                "signature" : {
                        "hash" : BinData(0,"AAAAAAAAAAAAAAAAAAAAAAAAAAA="),
                        "keyId" : NumberLong(0)
                }
        },
        "operationTime" : Timestamp(1716648503, 4)
}
>
>
